{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"VANE: Network Certification Tool","text":""},{"location":"index.html#description","title":"Description","text":"<p>Vane ( VA lidate NE twork)  is an open source, network validation tool designed to conduct tests on Arista's networking devices. It operates by establishing connections with the devices on a specified network, executing commands, and conducting tests against the generated output. By automating these tasks, Vane significantly streamlines the network validation process, sparing users from the time-consuming burden of repetitive testing that could otherwise span months. <p>Vane prioritizes user-friendliness, aiming to reduce the necessity for system operators to manually edit source code. It achieves this goal by dynamically conveying information to its test cases through YAML files, which serve as containers for parameters passed to specific test cases.</p> <p>A significant aspect of the tool is its versatility in reporting test case output across multiple formats such as JSON, HTML, Word documents, or Excel spreadsheets. This diverse range of output formats enhances the clarity of test case results and provides structured data, facilitating easier analysis of the test output.</p>"},{"location":"index.html#technologies-in-vane","title":"Technologies in Vane","text":"<p>Vane, is fundamentally a Python project, employing Python classes for tasks such as parsing command line arguments, configuring and executing tests, and reporting test output. Leveraging the versatility of Python, developers proficient in the language can effortlessly create or extend test cases within the Vane framework. <p>For reporting purposes, Vane integrates PyTest, capitalizing on its seamless execution of test cases and compatibility with existing functionality. All Vane test cases adhere to PyTest syntax. In essence, Vane serves as an enriched wrapper around PyTest, augmenting its capabilities. The ongoing focus of Arista's development efforts is geared towards enhancing user experience and simplifying the process of crafting test cases, and Vane aims to serve this. </p>"},{"location":"index.html#contributing","title":"Contributing","text":"Contributing pull requests are gladly welcomed for this repository. Please note that all contributions that modify the library behavior require corresponding test cases otherwise the pull request will be rejected."},{"location":"index.html#license","title":"License","text":"Copyright (c) 2023, Arista Networks EOS+ All rights reserved.  Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:   <ul> <li>Redistributions of source code must retain the above copyright notice, this   list of conditions and the following disclaimer.</li> <li>Redistributions in binary form must reproduce the above copyright notice,   this list of conditions and the following disclaimer in the documentation   and/or other materials provided with the distribution.</li> <li>Neither the name of the Arista nor the names of its   contributors may be used to endorse or promote products derived from   this software without specific prior written permission.</li> </ul> <p>Important</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"api_cli/api.html","title":"Vane API","text":""},{"location":"api_cli/api.html#vane.tests_tools","title":"<code>tests_tools</code>","text":"<p>This module has the TestOps class which provides an array of different operations that a test case can perform. It also consists of standalone functions which provide utility operations while executing test cases.</p>"},{"location":"api_cli/api.html#vane.tests_tools-classes","title":"Classes","text":""},{"location":"api_cli/api.html#vane.tests_tools.TestOps","title":"<code>TestOps</code>","text":"<p>The TestOps class introduces the API which lets you execute common testcase operations like running show commands on devices, generating test reports, writing evidence files and generating test steps. These operations get called from within the test case.</p> Source code in <code>vane/tests_tools.py</code> <pre><code>class TestOps:\n    \"\"\"The TestOps class introduces the API which lets you execute common testcase operations\n    like running show commands on devices, generating test reports, writing evidence files\n    and generating test steps. These operations get called from within the test case.\"\"\"\n\n    def __init__(self, tests_definitions, test_suite, dut):\n        \"\"\"Initializes the TestOps Object with test specific and dut specific data\n\n        Args:\n            tests_definitions (str): YAML representation of tests\n            test_suite (str): name of test suite\n            dut (dict): device under test\n        \"\"\"\n        test_case = inspect.stack()[1][3]\n        # Test cases that skip will change skip to True\n        self.skip = False\n        self.test_case = test_case\n        self.test_parameters = self._get_parameters(tests_definitions, test_suite, self.test_case)\n        self.expected_output = self.test_parameters[\"expected_output\"]\n        self.dut = dut\n        self.dut_name = self.dut[\"name\"]\n        self.interface_list = self.dut[\"output\"][\"interface_list\"]\n        self.results_dir = self.dut[\"results_dir\"]\n        self.report_dir = self.dut[\"report_dir\"]\n\n        parameters = config.test_parameters\n\n        try:\n            self.show_clock_flag = parameters[\"parameters\"][\"show_clock\"]\n        except KeyError:\n            self.show_clock_flag = False\n\n        self.show_cmds = {self.dut_name: []}\n        self._show_cmds = {self.dut_name: []}\n\n        if self.show_clock_flag:\n            self._show_cmds[self.dut_name].append(\"show clock\")\n\n        # add 'show version' to _show_cmds\n        self._show_cmds[self.dut_name].append(\"show version\")\n\n        self.show_output = \"\"\n        self.show_cmd = \"\"\n        self.test_steps = []\n        try:\n            self.show_cmd = self.test_parameters[\"show_cmd\"]\n            if self.show_cmd:\n                self.show_cmds[self.dut_name].append(self.show_cmd)\n                self._show_cmds[self.dut_name].append(self.show_cmd)\n        except KeyError:\n            self.show_cmds[self.dut_name].extend(self.test_parameters[\"show_cmds\"])\n            self._show_cmds[self.dut_name].extend(self.test_parameters[\"show_cmds\"])\n\n        self.show_cmd_txts = {self.dut_name: []}\n        self.show_cmd_txt = \"\"\n        self._show_cmd_txts = {self.dut_name: []}\n        self.external_cmd_txts = {}\n\n        if len(self._show_cmds[self.dut_name]) &gt; 0 and self.dut:\n            self._verify_show_cmd(self._show_cmds[self.dut_name], self.dut)\n            if self.show_cmd:\n                self.show_cmd_txt = self.dut[\"output\"][self.show_cmd][\"text\"]\n            for show_cmd in self.show_cmds[self.dut_name]:\n                self.show_cmd_txts[self.dut_name].append(self.dut[\"output\"][show_cmd][\"text\"])\n            for show_cmd in self._show_cmds[self.dut_name]:\n                self._show_cmd_txts[self.dut_name].append(self.dut[\"output\"][show_cmd][\"text\"])\n\n        self.comment = \"\"\n        self.output_msg = \"\"\n        self.actual_results = []\n        self.expected_results = []\n        self.actual_output = \"\"\n        self.test_result = False\n        self.test_id = self.test_parameters.get(\"test_id\", None)\n\n    def _verify_show_cmd(self, show_cmds, dut):\n        \"\"\"Verify if show command was successfully executed on dut\n\n        Args:\n            show_cmds (str): show command\n            dut (dict): data structure of dut parameters\n        \"\"\"\n        dut_name = dut[\"name\"]\n\n        logging.info(\n            f\"Verifying if show command {show_cmds} were successfully executed on {dut_name} dut\"\n        )\n\n        for show_cmd in show_cmds:\n            if show_cmd and show_cmd in dut[\"output\"]:\n                logging.debug(f\"Verified output for show command {show_cmd} on {dut_name}\")\n            else:\n                logging.critical(f\"Show command {show_cmd} not executed on {dut_name}\")\n\n                assert False\n\n    def _write_results(self):\n        \"\"\"Write the yaml output to a text file\"\"\"\n        logging.info(\"Preparing to write results\")\n\n        test_suite = self.test_parameters[\"test_suite\"]\n        test_suite = test_suite.split(\"/\")[-1]\n        dut_name = self.test_parameters[\"dut\"]\n        test_case = self.test_parameters[\"name\"]\n        results_dir = self.results_dir\n        yaml_file = f\"{results_dir}/result-{test_case}-{dut_name}.yml\"\n\n        logging.debug(f\"Creating results file named {yaml_file}\")\n\n        yaml_data = self.test_parameters\n        export_yaml(yaml_file, yaml_data)\n\n    def _write_text_results(self):\n        \"\"\"Write the text output of show command to a text file\"\"\"\n\n        self._write_evidence(self._show_cmds, self._show_cmd_txts, \"Verification\")\n\n    def _write_evidence(self, cmds, cmds_outputs, file_substring):\n        \"\"\"Write the cmds and their outputs to the file\n\n        Args:\n            cmds (dict): dictionary of dut names and show commands executed on that dut\n            cmds_outputs (dict): dictionary of dut names and outputs of show commands\n            file_substring (str): Type of file\n        \"\"\"\n\n        report_dir = self.report_dir\n        test_id = self.test_parameters[\"test_id\"]\n        test_case = self.test_parameters[\"name\"]\n\n        # write evidence for cmds if any\n        for dut_name, dut_cmds in cmds.items():\n            text_file = (\n                f\"{report_dir}/TEST RESULTS/{test_id} {test_case}/\"\n                f\"{test_id} {dut_name} {file_substring}.txt\"\n            )\n            text_data = {}\n            index = 1\n\n            for command, text in zip(dut_cmds, cmds_outputs[dut_name]):\n                text_data[str(index) + \". \" + dut_name + \"# \" + command] = \"\\n\\n\" + text\n                index += 1\n\n            if text_data:\n                export_text(text_file, text_data, self.dut_name)\n            else:\n                logging.debug(\n                    f\"No cfg command output to display for test id {test_id} test case {test_case}\"\n                )\n\n    def _get_parameters(self, tests_parameters, test_suite, test_case):\n        \"\"\"Return test parameters for a test case\n\n        Args:\n            tests_parameters (dict): Abstraction of testing parameters\n            test_suite (str): name of the test suite\n            test_case (str): name of the test case\n\n        Returns:\n            case_parameters (list): test parameters for a test case\n        \"\"\"\n        if not test_case:\n            test_case = inspect.stack()[1][3]\n\n            logging.info(f\"Setting testcase name to {test_case}\")\n\n        logging.info(\"Identify test case and return parameters\")\n\n        test_suite = test_suite.split(\"/\")[-1]\n\n        logging.debug(f\"Return testcases for Test Suite: {test_suite}\")\n\n        suite_parameters = [\n            copy.deepcopy(param)\n            for param in tests_parameters[\"test_suites\"]\n            if param[\"name\"] == test_suite\n        ]\n\n        logging.debug(f\"Suite_parameters: {suite_parameters}\")\n\n        logging.info(f\"Returning parameters for Test Case: {test_case}\")\n\n        case_parameters = [\n            copy.deepcopy(param)\n            for param in suite_parameters[0][\"testcases\"]\n            if param[\"name\"] == test_case\n        ]\n\n        logging.debug(f\"Case_parameters: {case_parameters[0]}\")\n\n        case_parameters[0][\"test_suite\"] = test_suite\n\n        return case_parameters[0]\n\n    def generate_report(self, dut_name, output=\"\"):\n        \"\"\"Utility to generate report\n\n        Args:\n          dut_name (str): name of the device\n          output (str): Output of commands executed on a device.Default value is\n                       an empty string.\n        \"\"\"\n        logging.debug(f\"Output on device {dut_name} after SSH connection is: {output}\")\n\n        self.test_parameters[\"comment\"] = self.comment\n        self.test_parameters[\"test_result\"] = self.test_result\n        self.test_parameters[\"output_msg\"] = self.output_msg\n        self.test_parameters[\"actual_output\"] = self.actual_output\n        self.test_parameters[\"expected_output\"] = self.expected_output\n        self.test_parameters[\"dut\"] = self.dut_name\n        self.test_parameters[\"show_cmd\"] = self.show_cmd\n        self.test_parameters[\"test_id\"] = self.test_id\n        self.test_parameters[\"show_cmd_txts\"] = self._show_cmd_txts\n        self.test_parameters[\"test_steps\"] = self.test_steps\n        self.test_parameters[\"show_cmds\"] = self._show_cmds\n        self.test_parameters[\"skip\"] = self.skip\n        self.test_parameters[\"external_command_outputs\"] = self.external_cmd_txts\n\n        if str(self.show_cmd_txt):\n            self.test_parameters[\"show_cmd\"] += \":\\n\\n\" + self.show_cmd_txt\n\n        self.test_parameters[\"test_id\"] = self.test_id\n        self.test_parameters[\"fail_or_skip_reason\"] = \"\"\n\n        if not self.test_parameters[\"test_result\"]:\n            self.test_parameters[\"fail_or_skip_reason\"] = self.output_msg\n\n        self._html_report()\n        self._write_results()\n        self._write_text_results()\n\n    def _html_report(self):\n        \"\"\"Print to standard output for HTML reporting\"\"\"\n\n        print(\"\\nOUTPUT MESSAGES:\")\n        print(\"================\")\n        print(f\"{self.output_msg}\\n{self.comment}\")\n\n        print(\"\\nEXPECTED OUTPUT:\")\n        print(\"================\")\n        pprint.pprint(self.expected_output)\n\n        print(\"\\n\\nACTUAL OUTPUT:\")\n        print(\"==============\")\n        pprint.pprint(self.actual_output)\n\n        print(\"\\n\\nSHOW OUTPUT COLLECTED IN TEST CASE:\")\n        print(\"===================================\")\n\n        for dut_index, (dut_name, _show_cmds) in enumerate(self._show_cmds.items(), start=1):\n            for cmd_index, (command, text) in enumerate(\n                zip(_show_cmds, self._show_cmd_txts[dut_name]), start=1\n            ):\n                print(f\"{dut_index}.{cmd_index}. {dut_name}# {command}\\n\\n{text}\")\n\n        if self.external_cmd_txts:\n            print(\"\\n\\nCOMMAND OUTPUT COLLECTED FROM EXTERNAL DEVICES IN TEST CASE:\")\n            print(\"===========================================================\")\n\n            for dut_index, (dut_name, output_details) in enumerate(\n                self.external_cmd_txts.items(), start=1\n            ):\n                for cmd_index, (cmd, output) in enumerate(output_details.items(), start=1):\n                    print(f\"{dut_index}.{cmd_index}. {dut_name}# {cmd}\\n\\n{output}\")\n\n    def parse_test_steps(self, func):\n        \"\"\"Returns a list of all the test steps in the given function.\n        Inspects functions and finds statements with TS: and organizes\n        them into a list.\n\n        Args:\n          func (obj): function reference with body to inspect for test steps\n        \"\"\"\n\n        # Extracting lines from the function\n        comments = []\n        lines, _ = inspect.getsourcelines(func)\n\n        # converting list of strings into a single string\n        content = \" \".join([str(elem) for elem in lines])\n\n        # Pattern to match to extract TS\n        pattern = re.compile('(TS:.*?)(?:\"\"\"|Args:)', re.DOTALL)\n\n        # Find all matches to pattern\n        comments = pattern.findall(content)\n\n        # Format each item in list\n        comments = [re.sub(r\"\\n\\s+\", \" \", x) for x in comments]\n\n        if not comments:\n            comments.append(\"N/a no Test Steps found\")\n\n        for step in comments:\n            # Add Test steps to list to be added to file\n            self.test_steps.append(step.lstrip(\"TS:\"))\n\n        logging.info(f\"These are test steps {self.test_steps}\")\n\n    def set_evidence_default(self, dut_name):\n        \"\"\"Initializes evidence values for neighbor duts since\n        init only initializes for primary dut\n\n        Args:\n            dut_name (str): Name of the dut\n        \"\"\"\n\n        self._show_cmd_txts.setdefault(dut_name, [])\n        self._show_cmds.setdefault(dut_name, [])\n        self.show_cmd_txts.setdefault(dut_name, [])\n\n    def get_ssh_connection(self, dut):\n        \"\"\"Return the ssh connection if it exists otherwise initialise\n        a new ssh connection\n\n        Args:\n            dut (dict): device whose ssh connection should be returned\n\n        Returns:\n            conn (netmiko connection): ssh connection for the device\"\"\"\n\n        if \"ssh_conn\" not in dut:\n            netmiko_conn = device_interface.NetmikoConn()\n            netmiko_conn.set_up_conn(dut)\n            dut[\"ssh_conn\"] = netmiko_conn\n            dut[\"connection\"] = netmiko_conn\n\n        return dut[\"ssh_conn\"]\n\n    def get_eapi_connection(self, dut):\n        \"\"\"Return the eapi connection if it exists otherwise initialise\n        a new eapi connection\n\n        Args:\n            dut (dict): device whose eapi connection should be returned\n\n        Returns:\n            conn (paramiko connection): eapi connection for the device\"\"\"\n\n        if \"eapi_conn\" not in dut:\n            pyeapi_conn = device_interface.PyeapiConn()\n            pyeapi_conn.set_up_conn(dut)\n            dut[\"eapi_conn\"] = pyeapi_conn\n            dut[\"connection\"] = pyeapi_conn\n\n        return dut[\"eapi_conn\"]\n\n    def get_new_conn(self, dut, conn_type, timeout):\n        \"\"\"Returns a new connection to the dut of type 'conn_type'\n        with read timeout set to timeout\n\n        Args:\n            dut (dict): the device to get the connection to\n            conn_type (pyeapi/netmiko conn): eapi or ssh\n            timeout (int): Read time out for the connection\n\n        Returns:\n            conn (pyeapi/netmiko): a new eapi or ssh connection to dut\n        \"\"\"\n        device_data = {}\n        device_data[\"transport\"] = dut[\"transport\"]\n        device_data[\"mgmt_ip\"] = dut[\"mgmt_ip\"]\n        device_data[\"username\"] = dut[\"username\"]\n        device_data[\"password\"] = dut[\"password\"]\n        device_data[\"enable_pwd\"] = dut.get(\"enable_pwd\", \"\")\n        device_data[\"timeout\"] = timeout\n        device_data[\"name\"] = dut[\"name\"]\n        if dut.get(\"session_log\"):\n            device_data[\"session_log\"] = dut[\"session_log\"]\n        if conn_type == \"eapi\":\n            logging.info(f\"Creating new eapi connection to {dut['name']}\")\n            pyeapi_conn = device_interface.PyeapiConn()\n            pyeapi_conn.set_up_conn(device_data)\n            return pyeapi_conn\n\n        if conn_type == \"ssh\":\n            logging.info(f\"Creating new ssh connection to {dut['name']}\")\n            netmiko_conn = device_interface.NetmikoConn()\n            netmiko_conn.set_up_conn(device_data)\n            return netmiko_conn\n\n        raise ValueError(f\"conn_type [{conn_type}] not supported\")\n\n    def run_cfg_cmds(self, cfg_cmds, dut=None, conn_type=\"eapi\", timeout=0, new_conn=False):\n        \"\"\"A wrapper which runs the configuration cmds\n        if no dut is passed then cmds are run on TestOps dut object,\n        if conn_type is eapi then pyeapi is used to connect to dut,\n        if conn_type is ssh then netmiko is used to connect to dut,\n        if timeout is non-zero then a new connection is created with new timeout,\n        if new_conn is True a new connection to dut is created.\n\n        Args:\n          cfg_cmds (list): list of configuration cmds to run\n          dut (dict): device on which cfg_cmds have to run\n          conn_type (pyeapi/netmiko): connection type to dut - either pyeapi or netmiko\n          timeout (int): read timeout for dut connection\n          new_conn (boolean): whether to get a new conn to dut\n\n        Returns:\n            obj (dict): A dict object that includes the response for each command\n        \"\"\"\n\n        return self._run_and_record_cmds(\n            encoding=\"text\",\n            cmd_type=\"cfg\",\n            cmds=cfg_cmds,\n            dut=dut,\n            conn_type=conn_type,\n            timeout=timeout,\n            new_conn=new_conn,\n        )\n\n    def run_show_cmds(\n        self,\n        show_cmds,\n        dut=None,\n        encoding=\"json\",\n        conn_type=\"eapi\",\n        timeout=0,\n        new_conn=False,\n        hidden_cmd=False,\n    ):\n        \"\"\"A wrapper which runs the 'show_cmds'\n        conn_type determines how the cmds are being run\n        if conn_type is eapi then pyeapi is used on specified dut,\n        if conn_type is ssh then netmiko connection in dut object is used\n        if no dut is passed then cmds are run on TestOps dut object.\n        It returns the output of these 'show_cmds' in the encoding requested.\n        Also it checks show_clock_flag\n        to see if 'show_clock' cmd needs to be run. It stores the text output for\n        'show_cmds' list in 'show_cmds_txt' list for the specific dut.\n        Also 'show_cmds' list is appended to object's 'show_cmds' list.\n        If timeout is non-zero then a new connection is created with new timeout.\n        If new_conn is set to True then new connection is created.\n\n        Args:\n          show_cmds (list): list of show commands to be run\n          dut (dict): the device to run the show command on\n          encoding (str): json or text, with json being default\n          conn_type (pyeapi/netmiko): eapi or ssh, with eapi being default\n          timeout (int): timeout to be used for connection to DUT\n          new_conn (boolean): whether or not to create a new conn to DUT\n\n        Returns:\n            obj (dict): A dict object that includes the response for each command along\n                        with the encoding\n        \"\"\"\n\n        return self._run_and_record_cmds(\n            encoding=encoding,\n            cmd_type=\"show\",\n            cmds=show_cmds,\n            dut=dut,\n            conn_type=conn_type,\n            timeout=timeout,\n            new_conn=new_conn,\n            hidden_cmd=hidden_cmd,\n        )\n\n    def _run_and_record_cmds(\n        self,\n        cmds,\n        conn_type,\n        timeout,\n        new_conn,\n        encoding=\"json\",\n        cmd_type=\"show\",\n        dut=None,\n        hidden_cmd=False,\n    ):\n        \"\"\"_run_and_record_cmds runs both config and show cmds and records the output\n        of these commands\n\n        Args:\n            cmds (list): list of cfg/show cmds to run\n            conn_type (pyeapi/netmiko conn): eapi or ssh\n            timeout (int): timeout to be used for connection to DUT, if non-zero timeout is\n                            specified then a new connection is created\n            new_conn (boolean): whether or not to create a new connection to DUT\n            encoding (str): json or text, with json being default\n            cmd_type (str): type of cmd to run - \"show\" or \"cfg\" with \"show\" being default\n            dut (dict): the device to run the cmds on\n\n        Returns:\n            obj (dict): A dict object that includes the response for each command\n        \"\"\"\n\n        # pylint: disable=no-member\n        # if dut is not passed, use this object's dut\n        if dut is None:\n            dut = self.dut\n\n        if timeout == 0:\n            # if timeout is zero, then use existing connections\n            if conn_type == \"eapi\":\n                conn = self.get_eapi_connection(dut)\n            elif conn_type == \"ssh\":\n                conn = self.get_ssh_connection(dut)\n            else:\n                raise ValueError(f\"conn_type [{conn_type}] not supported\")\n        elif timeout &gt; 0 or new_conn:\n            # if timeout is non-zero or user wants a new connection\n            # get the new connection\n            conn = self.get_new_conn(dut, conn_type, timeout)\n\n        dut_name = dut[\"name\"]\n\n        # initializing evidence values for other duts since\n        # init only initializes for primary dut\n\n        self.set_evidence_default(dut_name)\n\n        # first run show clock if flag is set\n        if self.show_clock_flag:\n            show_clock_cmds = [\"show clock\"]\n            # run the show_clock_cmds\n            try:\n                show_clock_op = conn.enable(show_clock_cmds, \"text\")\n            except BaseException as e:\n                # add the show clock cmd to _show_cmds evidence list\n                for cmd in show_clock_cmds:\n                    self._show_cmds[dut_name].append(cmd)\n                # add the exception result to _show_cmd_txts evidence output list\n                self._show_cmd_txts[dut_name].append(str(e))\n                raise e\n\n            # add the show_clock_cmds to _show_cmds list\n            # also add the o/p of show_clock_cmds to _show_cmd_txts list\n            for result_dict in show_clock_op:\n                self._show_cmds[dut_name].append(result_dict[\"command\"])\n                self._show_cmd_txts[dut_name].append(result_dict[\"result\"][\"output\"])\n\n        # then run commands\n        try:\n            if cmd_type == \"show\":\n                # see if hidden cmd, cmds might be jinja2 template\n                # render the cmds using dut object\n                run_cmds = cmds\n                if hidden_cmd:\n                    run_cmds = render_cmds(dut, cmds)\n\n                # run the commands in text mode first, to catch the evidence in case of\n                # command error.\n                txt_results = conn.enable(run_cmds, strict=True, encoding=\"text\")\n\n                # if encoding is json run the commands, store the results\n                if encoding == \"json\":\n                    json_results = conn.enable(run_cmds, strict=True)\n            else:\n                # run the config cmd\n                txt_results = conn.config(cmds)\n        except BaseException as e:  # pylint: disable=broad-except\n            logging.error(f\"Following cmds {cmds} generated exception {str(e)}\")\n            # add the cmds to _show_cmds cmds list\n            # add the exception result for all the cmds in the commands list.\n            # Skipped 1st command as it always an enable command and its output is\n            # an empty dictionary.\n            for index, cmd in enumerate(cmds, start=1):\n                self._show_cmds[dut_name].append(cmd)\n                if hidden_cmd:\n                    self._show_cmd_txts[dut_name].append(f\"{cmd} failed\")\n                    msg = f\"{cmd} failed to run. See logs for more details\"\n                    raise EapiError(message=msg) from e\n                error_output = None\n                if hasattr(e, \"output\"):\n                    error_output = e.output\n                if not hidden_cmd:\n                    # Collect output from the exception message.\n                    if error_output and cmd_type != \"cfg\":\n                        output = error_output[index].get(\"output\")\n                        self._show_cmd_txts[dut_name].append(output)\n                        error_msg = error_output[index].get(\"errors\")\n                        # Handled the command error occurred in one of the command\n                        # from the command list.\n                        if error_msg:\n                            msg = (\n                                f\"Failed to execute the command '{cmd}'.\"\n                                f\" Error: {output.replace('%', '',1)}\"\n                            )\n                            if hasattr(e, \"command_error\"):\n                                raise CommandError(code=e.error_code, message=msg) from e\n                            raise EapiError(message=msg) from e\n                    else:\n                        # Handled the scenario when an output error message in\n                        # exception not received\n                        self._show_cmd_txts[dut_name].append(str(e))\n                        raise e\n\n        # add the cmds to _show_cmds list\n        for cmd in cmds:\n            self._show_cmds[dut_name].append(cmd)\n\n        # also add the text o/p of cmds to _show_cmd_txts cmd output list\n        if cmd_type == \"cfg\" and conn_type == \"ssh\":\n            for cmd in cmds:\n                self._show_cmd_txts[dut_name].append(txt_results)\n        else:\n            for result_dict in txt_results:\n                result = result_dict.get(\"result\", {\"output\": \"\"})\n                self._show_cmd_txts[dut_name].append(result[\"output\"])\n\n        if cmd_type == \"show\" and encoding == \"json\":\n            return json_results\n\n        return txt_results\n\n    def transfer_file(self, src_file, dest_file, file_system, operation, dut=None, sftp=False):\n        \"\"\"Transfers filename to/from the the dut depending\n        on the operation mentioned.\n\n        Args:\n            dut (dict): device to/from which file needs to be transferred\n            src_file (str): full filename of src file\n            dest_file (str): full filename of dest file\n            operation (str): 'get' or 'put'\n            sftp (boolean): whether to use sftp transport or not\n\n        Returns:\n            result (dict): boolean values for file_exists, file_transferred and file_verified\n        \"\"\"\n\n        if dut is None:\n            dut = self.dut\n\n        dut_name = dut[\"name\"]\n\n        if operation not in (\"get\", \"put\"):\n            raise ValueError(f\"operation [{operation}] not supported\")\n\n        new_dut = dut.copy()\n        session_log = (\n            f\"netmiko-logs/file_transfer_{new_dut['name']}-{time.strftime('%Y%m%d-%H%M%S')}.log\"\n        )\n        new_dut[\"session_log\"] = session_log\n        conn = self.get_new_conn(new_dut, conn_type=\"ssh\", timeout=60)\n\n        # first run show clock if flag is set\n        if self.show_clock_flag:\n            show_clock_cmds = [\"show clock\"]\n            # run the show_clock_cmds\n            try:\n                show_clock_op = conn.enable(show_clock_cmds, \"text\")\n            except BaseException as e:\n                # add the show clock cmd to _show_cmds\n                for cmd in show_clock_cmds:\n                    self._show_cmds[dut_name].append(cmd)\n                    # add the exception result to _show_cmds_txts\n                    self._show_cmd_txts[dut_name].append(str(e))\n                raise e\n\n            # add the show_clock_cmds to internal cmds list\n            # also add the o/p of show_clock_cmds to external cmd output list\n            for result_dict in show_clock_op:\n                self._show_cmds[dut_name].append(result_dict[\"command\"])\n                self._show_cmd_txts[dut_name].append(result_dict[\"result\"][\"output\"])\n\n        if sftp:\n            cmd_str = \"sftp\"\n        else:\n            cmd_str = \"scp\"\n\n        # form request for evidence gathering\n        transfer_request = f\"{cmd_str} src_file: {src_file} dest_file: {dest_file} op: {operation}\"\n\n        # transfer file\n        try:\n            result = conn.transfer_file(src_file, dest_file, file_system, operation, sftp)\n        except BaseException as e:\n            self._show_cmds[new_dut[\"name\"]].append(transfer_request)\n            self._show_cmd_txts[new_dut[\"name\"]].append(str(e))\n            raise e\n\n        self._show_cmds[new_dut[\"name\"]].append(transfer_request)\n        # open session log and copy over the evidence\n        # hide the username from the evidence collection\n        with open(session_log, \"r\", encoding=\"utf-8\") as file:\n            self._show_cmd_txts[new_dut[\"name\"]].append(\n                file.read().replace(new_dut[\"username\"], \"XXXXX\")\n            )\n\n        try:\n            os.remove(session_log)\n        except OSError:\n            pass\n        return result\n\n    def setup_and_run_traffic(self, traffic_generator_type, configuration_file):\n        \"\"\"Module to call respective traffic generator based on the type of\n        traffic generator being used in the test case\n\n        Args:\n            traffic_generator_type (str): type of the traffic generator being used\n            configuration_file (.ixcng file): traffic profile file to pass to the traffic generator\n        \"\"\"\n\n        if traffic_generator_type == \"ixia\":\n            self.setup_ixia(configuration_file)\n\n    def setup_ixia(self, ixia_configuration):\n        \"\"\"Module to authenticate into Ixia Web Api, configure a session\n        with passed in configuration file, generate traffic and return\n        traffic and flow stats to validate test criteria\n\n        Args:\n            ixia_configuration (str): path of ixia config file\"\"\"\n\n        ixia_traffic_item_stats = []\n        self.traffic_item_stats = []\n        ixia_flow_stats = []\n        self.flow_stats = []\n        ix_network = None\n        session = None\n\n        try:\n            # Module 1 : Authentication: Connect to the IxNetwork API Server\n\n            session, ix_network = ixia_interface.authenticate()\n\n            # Module 2 : Configuration\n\n            ix_network = ixia_interface.configure(ix_network, ixia_configuration)\n\n            # Module 3 : Generating traffic\n\n            ix_network = ixia_interface.generate_traffic(ix_network)\n\n            # Get the traffic item and flow statistics\n\n            ixia_traffic_item_stats = StatViewAssistant(ix_network, \"Traffic Item Statistics\")\n\n            ixia_flow_stats = StatViewAssistant(ix_network, \"Flow Statistics\")\n\n            # Generate a deep copy of traffic and flow stats to store in tops object\n\n            index = 0\n            for traffic_item_stat in ixia_traffic_item_stats.Rows:\n                self.traffic_item_stats.append({})\n                for column, data in zip(traffic_item_stat.Columns, traffic_item_stat.RawData[0]):\n                    self.traffic_item_stats[index].update({column: data})\n                index += 1\n\n            index = 0\n            for flow_stat in ixia_flow_stats.Rows:\n                self.flow_stats.append({})\n                for column, data in zip(flow_stat.Columns, flow_stat.RawData[0]):\n                    self.flow_stats[index].update({column: data})\n                index += 1\n\n        except Exception as exception:  # pylint: disable=W0718\n            logging.error(\n                f\"Exception: Setting up of Ixia errored out due\"\n                f\" to the following reason: {format(exception)}\"\n            )\n\n        finally:\n            logging.info(\"Checking if there is a session to be cleared\")\n\n            if (ix_network and session) is not None:\n                ixia_interface.clear_session(ix_network, session)\n\n            else:\n                logging.info(\"No Session to clear\")\n\n    def add_cmds_evidence(self, cmds, cmds_output, device_name):\n        \"\"\"\n        API to update non-netmiko and non-pyeapi command execution outputs in the\n        the docx and html report.\n        The user must provide output in the desired format to this utility function.\n        The function does not handle the formatting of the output.\n\n        Args:\n            cmds (list): List of commands that are executed on the device.\n            cmds_output (list): List of command outputs corresponding to cmds.\n            device_name (str): Name of the device on which the commands were executed.\n        \"\"\"\n\n        # Checking whether the provided device is EOS device.\n        duts = config.test_duts\n        external_device = True\n        for device_details in duts[\"duts\"]:\n            if device_details.get(\"name\") == device_name:\n                external_device = False\n                break\n\n        if external_device:\n            # updating external commands and its outputs in the dictionary.\n            for cmd, cmd_output in zip(cmds, cmds_output):\n                self.external_cmd_txts.setdefault(device_name, {})\n                self.external_cmd_txts[device_name].update({cmd: cmd_output})\n\n        else:\n            # Initializing the evidence for a particular device.\n            self.set_evidence_default(device_name)\n\n            # Updating the commands and command output dictionaries.\n            self._show_cmds[device_name].extend(cmds)\n            self._show_cmd_txts[device_name].extend(cmds_output)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps-functions","title":"Functions","text":""},{"location":"api_cli/api.html#vane.tests_tools.TestOps.__init__","title":"<code>__init__</code>","text":"<p>Initializes the TestOps Object with test specific and dut specific data</p> <p>Parameters:</p> Name Type Description Default <code>tests_definitions</code> <code>str</code> <p>YAML representation of tests</p> required <code>test_suite</code> <code>str</code> <p>name of test suite</p> required <code>dut</code> <code>dict</code> <p>device under test</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def __init__(self, tests_definitions, test_suite, dut):\n    \"\"\"Initializes the TestOps Object with test specific and dut specific data\n\n    Args:\n        tests_definitions (str): YAML representation of tests\n        test_suite (str): name of test suite\n        dut (dict): device under test\n    \"\"\"\n    test_case = inspect.stack()[1][3]\n    # Test cases that skip will change skip to True\n    self.skip = False\n    self.test_case = test_case\n    self.test_parameters = self._get_parameters(tests_definitions, test_suite, self.test_case)\n    self.expected_output = self.test_parameters[\"expected_output\"]\n    self.dut = dut\n    self.dut_name = self.dut[\"name\"]\n    self.interface_list = self.dut[\"output\"][\"interface_list\"]\n    self.results_dir = self.dut[\"results_dir\"]\n    self.report_dir = self.dut[\"report_dir\"]\n\n    parameters = config.test_parameters\n\n    try:\n        self.show_clock_flag = parameters[\"parameters\"][\"show_clock\"]\n    except KeyError:\n        self.show_clock_flag = False\n\n    self.show_cmds = {self.dut_name: []}\n    self._show_cmds = {self.dut_name: []}\n\n    if self.show_clock_flag:\n        self._show_cmds[self.dut_name].append(\"show clock\")\n\n    # add 'show version' to _show_cmds\n    self._show_cmds[self.dut_name].append(\"show version\")\n\n    self.show_output = \"\"\n    self.show_cmd = \"\"\n    self.test_steps = []\n    try:\n        self.show_cmd = self.test_parameters[\"show_cmd\"]\n        if self.show_cmd:\n            self.show_cmds[self.dut_name].append(self.show_cmd)\n            self._show_cmds[self.dut_name].append(self.show_cmd)\n    except KeyError:\n        self.show_cmds[self.dut_name].extend(self.test_parameters[\"show_cmds\"])\n        self._show_cmds[self.dut_name].extend(self.test_parameters[\"show_cmds\"])\n\n    self.show_cmd_txts = {self.dut_name: []}\n    self.show_cmd_txt = \"\"\n    self._show_cmd_txts = {self.dut_name: []}\n    self.external_cmd_txts = {}\n\n    if len(self._show_cmds[self.dut_name]) &gt; 0 and self.dut:\n        self._verify_show_cmd(self._show_cmds[self.dut_name], self.dut)\n        if self.show_cmd:\n            self.show_cmd_txt = self.dut[\"output\"][self.show_cmd][\"text\"]\n        for show_cmd in self.show_cmds[self.dut_name]:\n            self.show_cmd_txts[self.dut_name].append(self.dut[\"output\"][show_cmd][\"text\"])\n        for show_cmd in self._show_cmds[self.dut_name]:\n            self._show_cmd_txts[self.dut_name].append(self.dut[\"output\"][show_cmd][\"text\"])\n\n    self.comment = \"\"\n    self.output_msg = \"\"\n    self.actual_results = []\n    self.expected_results = []\n    self.actual_output = \"\"\n    self.test_result = False\n    self.test_id = self.test_parameters.get(\"test_id\", None)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.add_cmds_evidence","title":"<code>add_cmds_evidence</code>","text":"<p>API to update non-netmiko and non-pyeapi command execution outputs in the the docx and html report. The user must provide output in the desired format to this utility function. The function does not handle the formatting of the output.</p> <p>Parameters:</p> Name Type Description Default <code>cmds</code> <code>list</code> <p>List of commands that are executed on the device.</p> required <code>cmds_output</code> <code>list</code> <p>List of command outputs corresponding to cmds.</p> required <code>device_name</code> <code>str</code> <p>Name of the device on which the commands were executed.</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def add_cmds_evidence(self, cmds, cmds_output, device_name):\n    \"\"\"\n    API to update non-netmiko and non-pyeapi command execution outputs in the\n    the docx and html report.\n    The user must provide output in the desired format to this utility function.\n    The function does not handle the formatting of the output.\n\n    Args:\n        cmds (list): List of commands that are executed on the device.\n        cmds_output (list): List of command outputs corresponding to cmds.\n        device_name (str): Name of the device on which the commands were executed.\n    \"\"\"\n\n    # Checking whether the provided device is EOS device.\n    duts = config.test_duts\n    external_device = True\n    for device_details in duts[\"duts\"]:\n        if device_details.get(\"name\") == device_name:\n            external_device = False\n            break\n\n    if external_device:\n        # updating external commands and its outputs in the dictionary.\n        for cmd, cmd_output in zip(cmds, cmds_output):\n            self.external_cmd_txts.setdefault(device_name, {})\n            self.external_cmd_txts[device_name].update({cmd: cmd_output})\n\n    else:\n        # Initializing the evidence for a particular device.\n        self.set_evidence_default(device_name)\n\n        # Updating the commands and command output dictionaries.\n        self._show_cmds[device_name].extend(cmds)\n        self._show_cmd_txts[device_name].extend(cmds_output)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.generate_report","title":"<code>generate_report</code>","text":"<p>Utility to generate report</p> <p>Parameters:</p> Name Type Description Default <code>dut_name</code> <code>str</code> <p>name of the device</p> required <code>output</code> <code>str</code> <p>Output of commands executed on a device.Default value is            an empty string.</p> <code>''</code> Source code in <code>vane/tests_tools.py</code> <pre><code>def generate_report(self, dut_name, output=\"\"):\n    \"\"\"Utility to generate report\n\n    Args:\n      dut_name (str): name of the device\n      output (str): Output of commands executed on a device.Default value is\n                   an empty string.\n    \"\"\"\n    logging.debug(f\"Output on device {dut_name} after SSH connection is: {output}\")\n\n    self.test_parameters[\"comment\"] = self.comment\n    self.test_parameters[\"test_result\"] = self.test_result\n    self.test_parameters[\"output_msg\"] = self.output_msg\n    self.test_parameters[\"actual_output\"] = self.actual_output\n    self.test_parameters[\"expected_output\"] = self.expected_output\n    self.test_parameters[\"dut\"] = self.dut_name\n    self.test_parameters[\"show_cmd\"] = self.show_cmd\n    self.test_parameters[\"test_id\"] = self.test_id\n    self.test_parameters[\"show_cmd_txts\"] = self._show_cmd_txts\n    self.test_parameters[\"test_steps\"] = self.test_steps\n    self.test_parameters[\"show_cmds\"] = self._show_cmds\n    self.test_parameters[\"skip\"] = self.skip\n    self.test_parameters[\"external_command_outputs\"] = self.external_cmd_txts\n\n    if str(self.show_cmd_txt):\n        self.test_parameters[\"show_cmd\"] += \":\\n\\n\" + self.show_cmd_txt\n\n    self.test_parameters[\"test_id\"] = self.test_id\n    self.test_parameters[\"fail_or_skip_reason\"] = \"\"\n\n    if not self.test_parameters[\"test_result\"]:\n        self.test_parameters[\"fail_or_skip_reason\"] = self.output_msg\n\n    self._html_report()\n    self._write_results()\n    self._write_text_results()\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.get_eapi_connection","title":"<code>get_eapi_connection</code>","text":"<p>Return the eapi connection if it exists otherwise initialise a new eapi connection</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>device whose eapi connection should be returned</p> required <p>Returns:</p> Name Type Description <code>conn</code> <code>paramiko connection</code> <p>eapi connection for the device</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def get_eapi_connection(self, dut):\n    \"\"\"Return the eapi connection if it exists otherwise initialise\n    a new eapi connection\n\n    Args:\n        dut (dict): device whose eapi connection should be returned\n\n    Returns:\n        conn (paramiko connection): eapi connection for the device\"\"\"\n\n    if \"eapi_conn\" not in dut:\n        pyeapi_conn = device_interface.PyeapiConn()\n        pyeapi_conn.set_up_conn(dut)\n        dut[\"eapi_conn\"] = pyeapi_conn\n        dut[\"connection\"] = pyeapi_conn\n\n    return dut[\"eapi_conn\"]\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.get_new_conn","title":"<code>get_new_conn</code>","text":"<p>Returns a new connection to the dut of type 'conn_type' with read timeout set to timeout</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>the device to get the connection to</p> required <code>conn_type</code> <code>pyeapi/netmiko conn</code> <p>eapi or ssh</p> required <code>timeout</code> <code>int</code> <p>Read time out for the connection</p> required <p>Returns:</p> Name Type Description <code>conn</code> <code>pyeapi / netmiko</code> <p>a new eapi or ssh connection to dut</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def get_new_conn(self, dut, conn_type, timeout):\n    \"\"\"Returns a new connection to the dut of type 'conn_type'\n    with read timeout set to timeout\n\n    Args:\n        dut (dict): the device to get the connection to\n        conn_type (pyeapi/netmiko conn): eapi or ssh\n        timeout (int): Read time out for the connection\n\n    Returns:\n        conn (pyeapi/netmiko): a new eapi or ssh connection to dut\n    \"\"\"\n    device_data = {}\n    device_data[\"transport\"] = dut[\"transport\"]\n    device_data[\"mgmt_ip\"] = dut[\"mgmt_ip\"]\n    device_data[\"username\"] = dut[\"username\"]\n    device_data[\"password\"] = dut[\"password\"]\n    device_data[\"enable_pwd\"] = dut.get(\"enable_pwd\", \"\")\n    device_data[\"timeout\"] = timeout\n    device_data[\"name\"] = dut[\"name\"]\n    if dut.get(\"session_log\"):\n        device_data[\"session_log\"] = dut[\"session_log\"]\n    if conn_type == \"eapi\":\n        logging.info(f\"Creating new eapi connection to {dut['name']}\")\n        pyeapi_conn = device_interface.PyeapiConn()\n        pyeapi_conn.set_up_conn(device_data)\n        return pyeapi_conn\n\n    if conn_type == \"ssh\":\n        logging.info(f\"Creating new ssh connection to {dut['name']}\")\n        netmiko_conn = device_interface.NetmikoConn()\n        netmiko_conn.set_up_conn(device_data)\n        return netmiko_conn\n\n    raise ValueError(f\"conn_type [{conn_type}] not supported\")\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.get_ssh_connection","title":"<code>get_ssh_connection</code>","text":"<p>Return the ssh connection if it exists otherwise initialise a new ssh connection</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>device whose ssh connection should be returned</p> required <p>Returns:</p> Name Type Description <code>conn</code> <code>netmiko connection</code> <p>ssh connection for the device</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def get_ssh_connection(self, dut):\n    \"\"\"Return the ssh connection if it exists otherwise initialise\n    a new ssh connection\n\n    Args:\n        dut (dict): device whose ssh connection should be returned\n\n    Returns:\n        conn (netmiko connection): ssh connection for the device\"\"\"\n\n    if \"ssh_conn\" not in dut:\n        netmiko_conn = device_interface.NetmikoConn()\n        netmiko_conn.set_up_conn(dut)\n        dut[\"ssh_conn\"] = netmiko_conn\n        dut[\"connection\"] = netmiko_conn\n\n    return dut[\"ssh_conn\"]\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.parse_test_steps","title":"<code>parse_test_steps</code>","text":"<p>Returns a list of all the test steps in the given function. Inspects functions and finds statements with TS: and organizes them into a list.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>obj</code> <p>function reference with body to inspect for test steps</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def parse_test_steps(self, func):\n    \"\"\"Returns a list of all the test steps in the given function.\n    Inspects functions and finds statements with TS: and organizes\n    them into a list.\n\n    Args:\n      func (obj): function reference with body to inspect for test steps\n    \"\"\"\n\n    # Extracting lines from the function\n    comments = []\n    lines, _ = inspect.getsourcelines(func)\n\n    # converting list of strings into a single string\n    content = \" \".join([str(elem) for elem in lines])\n\n    # Pattern to match to extract TS\n    pattern = re.compile('(TS:.*?)(?:\"\"\"|Args:)', re.DOTALL)\n\n    # Find all matches to pattern\n    comments = pattern.findall(content)\n\n    # Format each item in list\n    comments = [re.sub(r\"\\n\\s+\", \" \", x) for x in comments]\n\n    if not comments:\n        comments.append(\"N/a no Test Steps found\")\n\n    for step in comments:\n        # Add Test steps to list to be added to file\n        self.test_steps.append(step.lstrip(\"TS:\"))\n\n    logging.info(f\"These are test steps {self.test_steps}\")\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.run_cfg_cmds","title":"<code>run_cfg_cmds</code>","text":"<p>A wrapper which runs the configuration cmds if no dut is passed then cmds are run on TestOps dut object, if conn_type is eapi then pyeapi is used to connect to dut, if conn_type is ssh then netmiko is used to connect to dut, if timeout is non-zero then a new connection is created with new timeout, if new_conn is True a new connection to dut is created.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_cmds</code> <code>list</code> <p>list of configuration cmds to run</p> required <code>dut</code> <code>dict</code> <p>device on which cfg_cmds have to run</p> <code>None</code> <code>conn_type</code> <code>pyeapi / netmiko</code> <p>connection type to dut - either pyeapi or netmiko</p> <code>'eapi'</code> <code>timeout</code> <code>int</code> <p>read timeout for dut connection</p> <code>0</code> <code>new_conn</code> <code>boolean</code> <p>whether to get a new conn to dut</p> <code>False</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>dict</code> <p>A dict object that includes the response for each command</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def run_cfg_cmds(self, cfg_cmds, dut=None, conn_type=\"eapi\", timeout=0, new_conn=False):\n    \"\"\"A wrapper which runs the configuration cmds\n    if no dut is passed then cmds are run on TestOps dut object,\n    if conn_type is eapi then pyeapi is used to connect to dut,\n    if conn_type is ssh then netmiko is used to connect to dut,\n    if timeout is non-zero then a new connection is created with new timeout,\n    if new_conn is True a new connection to dut is created.\n\n    Args:\n      cfg_cmds (list): list of configuration cmds to run\n      dut (dict): device on which cfg_cmds have to run\n      conn_type (pyeapi/netmiko): connection type to dut - either pyeapi or netmiko\n      timeout (int): read timeout for dut connection\n      new_conn (boolean): whether to get a new conn to dut\n\n    Returns:\n        obj (dict): A dict object that includes the response for each command\n    \"\"\"\n\n    return self._run_and_record_cmds(\n        encoding=\"text\",\n        cmd_type=\"cfg\",\n        cmds=cfg_cmds,\n        dut=dut,\n        conn_type=conn_type,\n        timeout=timeout,\n        new_conn=new_conn,\n    )\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.run_show_cmds","title":"<code>run_show_cmds</code>","text":"<p>A wrapper which runs the 'show_cmds' conn_type determines how the cmds are being run if conn_type is eapi then pyeapi is used on specified dut, if conn_type is ssh then netmiko connection in dut object is used if no dut is passed then cmds are run on TestOps dut object. It returns the output of these 'show_cmds' in the encoding requested. Also it checks show_clock_flag to see if 'show_clock' cmd needs to be run. It stores the text output for 'show_cmds' list in 'show_cmds_txt' list for the specific dut. Also 'show_cmds' list is appended to object's 'show_cmds' list. If timeout is non-zero then a new connection is created with new timeout. If new_conn is set to True then new connection is created.</p> <p>Parameters:</p> Name Type Description Default <code>show_cmds</code> <code>list</code> <p>list of show commands to be run</p> required <code>dut</code> <code>dict</code> <p>the device to run the show command on</p> <code>None</code> <code>encoding</code> <code>str</code> <p>json or text, with json being default</p> <code>'json'</code> <code>conn_type</code> <code>pyeapi / netmiko</code> <p>eapi or ssh, with eapi being default</p> <code>'eapi'</code> <code>timeout</code> <code>int</code> <p>timeout to be used for connection to DUT</p> <code>0</code> <code>new_conn</code> <code>boolean</code> <p>whether or not to create a new conn to DUT</p> <code>False</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>dict</code> <p>A dict object that includes the response for each command along         with the encoding</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def run_show_cmds(\n    self,\n    show_cmds,\n    dut=None,\n    encoding=\"json\",\n    conn_type=\"eapi\",\n    timeout=0,\n    new_conn=False,\n    hidden_cmd=False,\n):\n    \"\"\"A wrapper which runs the 'show_cmds'\n    conn_type determines how the cmds are being run\n    if conn_type is eapi then pyeapi is used on specified dut,\n    if conn_type is ssh then netmiko connection in dut object is used\n    if no dut is passed then cmds are run on TestOps dut object.\n    It returns the output of these 'show_cmds' in the encoding requested.\n    Also it checks show_clock_flag\n    to see if 'show_clock' cmd needs to be run. It stores the text output for\n    'show_cmds' list in 'show_cmds_txt' list for the specific dut.\n    Also 'show_cmds' list is appended to object's 'show_cmds' list.\n    If timeout is non-zero then a new connection is created with new timeout.\n    If new_conn is set to True then new connection is created.\n\n    Args:\n      show_cmds (list): list of show commands to be run\n      dut (dict): the device to run the show command on\n      encoding (str): json or text, with json being default\n      conn_type (pyeapi/netmiko): eapi or ssh, with eapi being default\n      timeout (int): timeout to be used for connection to DUT\n      new_conn (boolean): whether or not to create a new conn to DUT\n\n    Returns:\n        obj (dict): A dict object that includes the response for each command along\n                    with the encoding\n    \"\"\"\n\n    return self._run_and_record_cmds(\n        encoding=encoding,\n        cmd_type=\"show\",\n        cmds=show_cmds,\n        dut=dut,\n        conn_type=conn_type,\n        timeout=timeout,\n        new_conn=new_conn,\n        hidden_cmd=hidden_cmd,\n    )\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.set_evidence_default","title":"<code>set_evidence_default</code>","text":"<p>Initializes evidence values for neighbor duts since init only initializes for primary dut</p> <p>Parameters:</p> Name Type Description Default <code>dut_name</code> <code>str</code> <p>Name of the dut</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def set_evidence_default(self, dut_name):\n    \"\"\"Initializes evidence values for neighbor duts since\n    init only initializes for primary dut\n\n    Args:\n        dut_name (str): Name of the dut\n    \"\"\"\n\n    self._show_cmd_txts.setdefault(dut_name, [])\n    self._show_cmds.setdefault(dut_name, [])\n    self.show_cmd_txts.setdefault(dut_name, [])\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.setup_and_run_traffic","title":"<code>setup_and_run_traffic</code>","text":"<p>Module to call respective traffic generator based on the type of traffic generator being used in the test case</p> <p>Parameters:</p> Name Type Description Default <code>traffic_generator_type</code> <code>str</code> <p>type of the traffic generator being used</p> required <code>configuration_file</code> <code>.ixcng file</code> <p>traffic profile file to pass to the traffic generator</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def setup_and_run_traffic(self, traffic_generator_type, configuration_file):\n    \"\"\"Module to call respective traffic generator based on the type of\n    traffic generator being used in the test case\n\n    Args:\n        traffic_generator_type (str): type of the traffic generator being used\n        configuration_file (.ixcng file): traffic profile file to pass to the traffic generator\n    \"\"\"\n\n    if traffic_generator_type == \"ixia\":\n        self.setup_ixia(configuration_file)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.setup_ixia","title":"<code>setup_ixia</code>","text":"<p>Module to authenticate into Ixia Web Api, configure a session with passed in configuration file, generate traffic and return traffic and flow stats to validate test criteria</p> <p>Parameters:</p> Name Type Description Default <code>ixia_configuration</code> <code>str</code> <p>path of ixia config file</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def setup_ixia(self, ixia_configuration):\n    \"\"\"Module to authenticate into Ixia Web Api, configure a session\n    with passed in configuration file, generate traffic and return\n    traffic and flow stats to validate test criteria\n\n    Args:\n        ixia_configuration (str): path of ixia config file\"\"\"\n\n    ixia_traffic_item_stats = []\n    self.traffic_item_stats = []\n    ixia_flow_stats = []\n    self.flow_stats = []\n    ix_network = None\n    session = None\n\n    try:\n        # Module 1 : Authentication: Connect to the IxNetwork API Server\n\n        session, ix_network = ixia_interface.authenticate()\n\n        # Module 2 : Configuration\n\n        ix_network = ixia_interface.configure(ix_network, ixia_configuration)\n\n        # Module 3 : Generating traffic\n\n        ix_network = ixia_interface.generate_traffic(ix_network)\n\n        # Get the traffic item and flow statistics\n\n        ixia_traffic_item_stats = StatViewAssistant(ix_network, \"Traffic Item Statistics\")\n\n        ixia_flow_stats = StatViewAssistant(ix_network, \"Flow Statistics\")\n\n        # Generate a deep copy of traffic and flow stats to store in tops object\n\n        index = 0\n        for traffic_item_stat in ixia_traffic_item_stats.Rows:\n            self.traffic_item_stats.append({})\n            for column, data in zip(traffic_item_stat.Columns, traffic_item_stat.RawData[0]):\n                self.traffic_item_stats[index].update({column: data})\n            index += 1\n\n        index = 0\n        for flow_stat in ixia_flow_stats.Rows:\n            self.flow_stats.append({})\n            for column, data in zip(flow_stat.Columns, flow_stat.RawData[0]):\n                self.flow_stats[index].update({column: data})\n            index += 1\n\n    except Exception as exception:  # pylint: disable=W0718\n        logging.error(\n            f\"Exception: Setting up of Ixia errored out due\"\n            f\" to the following reason: {format(exception)}\"\n        )\n\n    finally:\n        logging.info(\"Checking if there is a session to be cleared\")\n\n        if (ix_network and session) is not None:\n            ixia_interface.clear_session(ix_network, session)\n\n        else:\n            logging.info(\"No Session to clear\")\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.TestOps.transfer_file","title":"<code>transfer_file</code>","text":"<p>Transfers filename to/from the the dut depending on the operation mentioned.</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>device to/from which file needs to be transferred</p> <code>None</code> <code>src_file</code> <code>str</code> <p>full filename of src file</p> required <code>dest_file</code> <code>str</code> <p>full filename of dest file</p> required <code>operation</code> <code>str</code> <p>'get' or 'put'</p> required <code>sftp</code> <code>boolean</code> <p>whether to use sftp transport or not</p> <code>False</code> <p>Returns:</p> Name Type Description <code>result</code> <code>dict</code> <p>boolean values for file_exists, file_transferred and file_verified</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def transfer_file(self, src_file, dest_file, file_system, operation, dut=None, sftp=False):\n    \"\"\"Transfers filename to/from the the dut depending\n    on the operation mentioned.\n\n    Args:\n        dut (dict): device to/from which file needs to be transferred\n        src_file (str): full filename of src file\n        dest_file (str): full filename of dest file\n        operation (str): 'get' or 'put'\n        sftp (boolean): whether to use sftp transport or not\n\n    Returns:\n        result (dict): boolean values for file_exists, file_transferred and file_verified\n    \"\"\"\n\n    if dut is None:\n        dut = self.dut\n\n    dut_name = dut[\"name\"]\n\n    if operation not in (\"get\", \"put\"):\n        raise ValueError(f\"operation [{operation}] not supported\")\n\n    new_dut = dut.copy()\n    session_log = (\n        f\"netmiko-logs/file_transfer_{new_dut['name']}-{time.strftime('%Y%m%d-%H%M%S')}.log\"\n    )\n    new_dut[\"session_log\"] = session_log\n    conn = self.get_new_conn(new_dut, conn_type=\"ssh\", timeout=60)\n\n    # first run show clock if flag is set\n    if self.show_clock_flag:\n        show_clock_cmds = [\"show clock\"]\n        # run the show_clock_cmds\n        try:\n            show_clock_op = conn.enable(show_clock_cmds, \"text\")\n        except BaseException as e:\n            # add the show clock cmd to _show_cmds\n            for cmd in show_clock_cmds:\n                self._show_cmds[dut_name].append(cmd)\n                # add the exception result to _show_cmds_txts\n                self._show_cmd_txts[dut_name].append(str(e))\n            raise e\n\n        # add the show_clock_cmds to internal cmds list\n        # also add the o/p of show_clock_cmds to external cmd output list\n        for result_dict in show_clock_op:\n            self._show_cmds[dut_name].append(result_dict[\"command\"])\n            self._show_cmd_txts[dut_name].append(result_dict[\"result\"][\"output\"])\n\n    if sftp:\n        cmd_str = \"sftp\"\n    else:\n        cmd_str = \"scp\"\n\n    # form request for evidence gathering\n    transfer_request = f\"{cmd_str} src_file: {src_file} dest_file: {dest_file} op: {operation}\"\n\n    # transfer file\n    try:\n        result = conn.transfer_file(src_file, dest_file, file_system, operation, sftp)\n    except BaseException as e:\n        self._show_cmds[new_dut[\"name\"]].append(transfer_request)\n        self._show_cmd_txts[new_dut[\"name\"]].append(str(e))\n        raise e\n\n    self._show_cmds[new_dut[\"name\"]].append(transfer_request)\n    # open session log and copy over the evidence\n    # hide the username from the evidence collection\n    with open(session_log, \"r\", encoding=\"utf-8\") as file:\n        self._show_cmd_txts[new_dut[\"name\"]].append(\n            file.read().replace(new_dut[\"username\"], \"XXXXX\")\n        )\n\n    try:\n        os.remove(session_log)\n    except OSError:\n        pass\n    return result\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools-functions","title":"Functions","text":""},{"location":"api_cli/api.html#vane.tests_tools.authenticate_and_setup_conn","title":"<code>authenticate_and_setup_conn</code>","text":"<p>Method to setup and authenticate setting up PyEapi or Netmiko connection based on conn object passed</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>device data</p> required <code>conn_object</code> <code>pyeapi / netmiko</code> <p>type of connection</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def authenticate_and_setup_conn(dut, conn_object):\n    \"\"\"Method to setup and authenticate setting up\n    PyEapi or Netmiko connection based on conn object passed\n\n    Args:\n        dut (dict): device data\n        conn_object (pyeapi/netmiko): type of connection\n    \"\"\"\n    dut_name = dut[\"name\"]\n    try:\n        conn_object.set_up_conn(dut)\n    except (ConnectionError, NetmikoAuthenticationException) as err:\n        try:\n            continue_when_unreachable = config.test_parameters[\"parameters\"][\n                \"continue_when_unreachable\"\n            ]\n        except KeyError:\n            continue_when_unreachable = False\n        if not continue_when_unreachable:\n            print(\n                \"\\x1b[31mExiting Vane.\\n\"\n                f\"Error running all cmds on dut {dut_name} due to failed authentication.\\n{err}\\n\"\n                \"\\x1b[0m\"\n            )\n            logging.error(\n                \"Exiting Vane: \"\n                f\"Error running all cmds on dut {dut_name} due to failed authentication. {err}\\n\"\n            )\n            sys.exit(1)\n        else:\n            logging.info(f\"Authentication to dut {dut_name} failed\")\n            return False\n\n    logging.info(f\"Authentication to dut {dut_name} is successful\")\n    return True\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.check_duts_reachability","title":"<code>check_duts_reachability</code>","text":"<p>Check if duts are reachable</p> <p>Parameters:</p> Name Type Description Default <code>test_duts</code> <code>dict</code> <p>Dictionary of duts</p> required <p>Returns:</p> Name Type Description <code>reachability</code> <code>boolean</code> <p>result of if duts are reachable</p> <code>reachable_duts</code> <code>dict</code> <p>reachable duts</p> <code>unreachable_duts</code> <code>dict</code> <p>unreachable duts</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def check_duts_reachability(test_duts):\n    \"\"\"Check if duts are reachable\n\n    Args:\n        test_duts (dict): Dictionary of duts\n\n    Returns:\n        reachability (boolean): result of if duts are reachable\n        reachable_duts (dict): reachable duts\n        unreachable_duts (dict): unreachable duts\n    \"\"\"\n\n    logging.info(\"Checking connectivity of duts\")\n    reachable_duts = []\n    unreachable_duts = []\n    ret = False\n    for dut in test_duts[\"duts\"]:\n        # check for reachability\n        ip_address = dut[\"mgmt_ip\"]\n        try:\n            host = ping(ip_address, count=3, interval=1, timeout=3, privileged=False)\n            ret = host.is_alive\n        except SocketPermissionError as e:\n            logging.error(\n                f\"Entered the exception due to permission issues: {e}\\n\"\n                \"Trying the ping utility via os.system instead\"\n            )\n            host = os.system(f\"ping -c 1 -W 3 {ip_address} &gt; {os.devnull}\")\n            ret = host == 0\n\n        if ret:\n            reachable_duts.append(dut)\n        else:\n            name = dut[\"name\"]\n            logging.info(f\"Failed to connect to {name}\")\n            unreachable_duts.append(dut)\n\n    if len(test_duts[\"duts\"]) == len(reachable_duts):\n        return True, reachable_duts, unreachable_duts\n\n    return False, reachable_duts, unreachable_duts\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.create_duts_file","title":"<code>create_duts_file</code>","text":"<p>Automatically generate a DUTs file</p> <p>Parameters:</p> Name Type Description Default <code>topology_file</code> <code>str</code> <p>Name and path of topology file</p> required <code>inventory_file</code> <code>str</code> <p>Name and path of inventory file</p> required <p>Returns:</p> Name Type Description <code>filename</code> <code>str</code> <p>duts file name</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def create_duts_file(topology_file, inventory_file, duts_file_name):\n    \"\"\"Automatically generate a DUTs file\n\n    Args:\n        topology_file (str): Name and path of topology file\n        inventory_file (str): Name and path of inventory file\n\n    Returns:\n        filename (str): duts file name\n    \"\"\"\n    dut_file = {}\n    dut_properties = []\n    topology_file = import_yaml(topology_file)\n    inventory_file = import_yaml(inventory_file)\n\n    try:\n        if not topology_file.get(\"nodes\", None):\n            inventory_file, topology_file = topology_file, inventory_file\n        for node in topology_file[\"nodes\"]:\n            name, topology_details = list(node.items())[0]\n            if \"cvp\" in name:\n                continue\n            if name in inventory_file[\"all\"][\"children\"][\"VEOS\"][\"hosts\"]:\n                inventory_details = inventory_file[\"all\"][\"children\"][\"VEOS\"][\"hosts\"][name]\n                dut_properties.append(\n                    {\n                        \"mgmt_ip\": inventory_details[\"ansible_host\"],\n                        \"name\": name,\n                        \"neighbors\": topology_details[\"neighbors\"],\n                        \"password\": inventory_details[\"ansible_ssh_pass\"],\n                        \"transport\": \"https\",\n                        \"username\": inventory_details[\"ansible_user\"],\n                        \"role\": topology_details.get(\"role\", \"unknown\"),\n                    }\n                )\n            else:\n                continue\n\n        if dut_properties:\n            dut_file.update({\"duts\": dut_properties})\n            with open(duts_file_name, \"w\", encoding=\"utf-8\") as yamlfile:\n                yaml.dump(dut_file, yamlfile, sort_keys=False)\n\n    # pylint: disable-next=broad-exception-caught\n    except Exception as excep:\n        logging.error(f\"Error occurred while creating DUTs file: {str(excep)}\")\n        logging.error(\"EXITING TEST RUNNER\")\n        print(\"&gt;&gt;&gt; ERROR While creating duts file\")\n        sys.exit(1)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.dut_worker","title":"<code>dut_worker</code>","text":"<p>Execute inputted show commands on dut.  Update dut structured data with show output.</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>structured data of a dut output data, hostname, and</p> required <code>show_cmds</code> <code>list</code> <p>List of show commands</p> required <code>reachable_duts</code> <code>dict</code> <p>Abstraction of duts</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def dut_worker(dut, show_cmds, reachable_duts):\n    \"\"\"Execute inputted show commands on dut.  Update dut structured data\n    with show output.\n\n    Args:\n      dut (dict): structured data of a dut output data, hostname, and\n      show_cmds (list): List of show commands\n      reachable_duts (dict): Abstraction of duts\n    \"\"\"\n    name = dut[\"name\"]\n    conn = dut[\"connection\"]\n    dut[\"output\"] = {}\n    dut[\"output\"][\"interface_list\"] = return_interfaces(name, reachable_duts)\n\n    logging.info(f\"Executing show commands on {name}\")\n    logging.debug(f\"List of show commands {show_cmds}\")\n\n    all_cmds_json = show_cmds.copy()\n    show_cmd_json_list, show_cmds_json = send_cmds(all_cmds_json, conn, \"json\")\n\n    logging.debug(f\"Returned from send_cmds_json {show_cmds_json}\")\n\n    all_cmds_txt = show_cmds.copy()\n    show_cmd_txt_list, show_cmds_txt = send_cmds(all_cmds_txt, conn, \"text\")\n\n    logging.debug(f\"Returned from send_cmds_txt {show_cmds_txt}\")\n\n    for show_cmd in show_cmds:\n        function_def = f'test_{(\"_\").join(show_cmd.split())}'\n\n        logging.debug(f\"Executing show command: {show_cmd} for test {function_def}\")\n        logging.debug(f\"Adding output of {show_cmd} to duts data structure\")\n\n        dut[\"output\"][show_cmd] = {}\n\n        if show_cmd in show_cmds_json:\n            cmd_index = show_cmds_json.index(show_cmd)\n\n            logging.debug(f\"Found cmd: {show_cmd} at index {cmd_index} of {show_cmds_json}\")\n            logging.debug(\n                f\"length of cmds: {len(show_cmds_json)} vs length of \"\n                f\"output {len(show_cmd_json_list)}\"\n            )\n\n            show_output = show_cmd_json_list[cmd_index]\n            dut[\"output\"][show_cmd][\"json\"] = show_output\n\n            logging.debug(f\"Adding cmd {show_cmd} to dut and data {show_output}\")\n        else:\n            dut[\"output\"][show_cmd][\"json\"] = \"\"\n\n            logging.debug(f\"No json output for {show_cmd}\")\n\n        if show_cmd in show_cmds_txt:\n            cmd_index = show_cmds_txt.index(show_cmd)\n\n            logging.debug(f\"Found cmd: {show_cmd} at index {cmd_index} of {show_cmds_txt}\")\n            logging.debug(\n                f\"length of cmds: {len(show_cmds_txt)} vs length of \"\n                f\"output {len(show_cmd_txt_list)}\"\n            )\n\n            show_output_txt = show_cmd_txt_list[cmd_index][\"output\"]\n            dut[\"output\"][show_cmd][\"text\"] = show_output_txt\n\n            logging.debug(f\"Adding cmd {show_cmd} to dut and data {show_output_txt}\")\n\n        else:\n            dut[\"output\"][show_cmd][\"text\"] = \"\"\n\n            logging.debug(f\"No text output for {show_cmd}\")\n\n    logging.info(f\"{name} is updated with show output.\")\n    logging.debug(f\"{name} updated with show output {dut}\")\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.export_text","title":"<code>export_text</code>","text":"<p>Export python data structure as a TEXT file</p> <p>Parameters:</p> Name Type Description Default <code>text_file</code> <code>str</code> <p>Name of TEXT file</p> required <code>text_data</code> <code>dict</code> <p>output of show command in python dictionary</p> required <code>dut_name</code> <code>str</code> <p>Primary dut name</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def export_text(text_file, text_data, dut_name):\n    \"\"\"Export python data structure as a TEXT file\n\n    Args:\n        text_file (str): Name of TEXT file\n        text_data (dict): output of show command in python dictionary\n        dut_name (str): Primary dut name\n    \"\"\"\n    logging.info(f\"Opening {text_file} for write\")\n\n    # to create the sub-directory if it does not exist\n    os.makedirs(os.path.dirname(text_file), exist_ok=True)\n\n    try:\n        with open(text_file, \"a\", encoding=\"utf-8\") as text_out:\n            logging.debug(f\"Output the following text file: {text_data}\")\n            divider = \"================================================================\"\n            heading = (\n                f\"{divider}\\nThese commands were run when PRIMARY DUT was {dut_name}\\n{divider}\\n\\n\"\n            )\n            text_out.write(heading)\n            for key, value in text_data.items():\n                text_out.write(f\"{key}{value}\\n\")\n    except OSError as err:\n        print(f\"&gt;&gt;&gt; {text_file} TEXT FILE MISSING\")\n        logging.error(f\"ERROR TEXT FILE: {text_file} NOT FOUND. {err}\")\n        logging.error(\"EXITING TEST RUNNER\")\n        sys.exit(1)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.export_yaml","title":"<code>export_yaml</code>","text":"<p>Export python data structure as a YAML file</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>str</code> <p>Name of YAML file</p> required <code>yaml_data</code> <code>dict</code> <p>Data to be written to yaml file</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def export_yaml(yaml_file, yaml_data):\n    \"\"\"Export python data structure as a YAML file\n\n    Args:\n        yaml_file (str): Name of YAML file\n        yaml_data (dict): Data to be written to yaml file\n    \"\"\"\n    logging.info(f\"Opening {yaml_file} for write\")\n\n    try:\n        with open(yaml_file, \"w\", encoding=\"utf-8\") as yaml_out:\n            try:\n                logging.debug(f\"Output the following yaml: {yaml_data}\")\n\n                yaml.dump(yaml_data, yaml_out, default_flow_style=False)\n            except yaml.YAMLError as err:\n                print(\"&gt;&gt;&gt; ERROR IN YAML FILE\")\n                logging.error(f\"ERROR IN YAML FILE: {err}\")\n                logging.error(\"EXITING TEST RUNNER\")\n                sys.exit(1)\n    except OSError as err:\n        print(f\"&gt;&gt;&gt; {yaml_file} YAML FILE MISSING\")\n        logging.error(f\"ERROR YAML FILE: {yaml_file} NOT \" + f\"FOUND. {err}\")\n        logging.error(\"EXITING TEST RUNNER\")\n        sys.exit(1)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.filter_duts","title":"<code>filter_duts</code>","text":"<p>Filter duts based on a user provided criteria and a filter</p> <p>Parameters:</p> Name Type Description Default <code>duts</code> <code>dict</code> <p>Full global duts dictionary</p> required <code>criteria</code> <code>str</code> <p>Type of filtering required. Valid options             are name, role, regex, or names. Defaults to \"\".</p> <code>''</code> <code>dut_filter</code> <code>str</code> <p>Filter for DUTs. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>subset_duts</code> <code>list(dict)</code> <p>Filtered subset of global dictionary of duts</p> <code>dut_names</code> <code>list(str)</code> <p>Filtered subset of global dictionary of dut names</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def filter_duts(duts, criteria=\"\", dut_filter=\"\"):\n    \"\"\"Filter duts based on a user provided criteria and a filter\n\n    Args:\n        duts (dict): Full global duts dictionary\n        criteria (str): Type of filtering required. Valid options\n                        are name, role, regex, or names. Defaults to \"\".\n        dut_filter (str): Filter for DUTs. Defaults to \"\".\n\n    Returns:\n        subset_duts (list(dict)): Filtered subset of global dictionary of duts\n        dut_names (list(str)): Filtered subset of global dictionary of dut names\n    \"\"\"\n    logging.info(f\"Filter: {dut_filter} by criteria: {criteria}\")\n\n    subset_duts, dut_names = [], []\n    if criteria == \"roles\":\n        for role in dut_filter:\n            subset_duts = subset_duts + [dut for dut in duts if role == dut[\"role\"]]\n            dut_names = dut_names + [dut[\"name\"] for dut in duts if role == dut[\"role\"]]\n    elif criteria == \"names\":\n        for name in dut_filter:\n            subset_duts = subset_duts + [dut for dut in duts if name == dut[\"name\"]]\n            dut_names = dut_names + [dut[\"name\"] for dut in duts if name == dut[\"name\"]]\n    elif criteria == \"regex\":\n        subset_duts = [dut for dut in duts if re.match(dut_filter, dut[\"name\"])]\n        dut_names = [dut[\"name\"] for dut in duts if re.match(dut_filter, dut[\"name\"])]\n    else:\n        subset_duts = duts\n        dut_names = [dut[\"name\"] for dut in duts]\n\n    return subset_duts, dut_names\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.get_parameters","title":"<code>get_parameters</code>","text":"<p>Return test parameters for a test case</p> <p>Parameters:</p> Name Type Description Default <code>tests_parameters</code> <code>dict</code> <p>Abstraction of testing parameters</p> required <code>test_suite</code> <code>str</code> <p>test suite of the test case</p> required <p>Returns:</p> Name Type Description <code>case_parameters</code> <code>list</code> <p>test parameters for a test case</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def get_parameters(tests_parameters, test_suite, test_case=\"\"):\n    \"\"\"Return test parameters for a test case\n\n    Args:\n        tests_parameters (dict): Abstraction of testing parameters\n        test_suite (str): test suite of the test case\n\n    Returns:\n        case_parameters (list): test parameters for a test case\n    \"\"\"\n    if not test_case:\n        test_case = inspect.stack()[1][3]\n\n        logging.info(f\"Setting testcase name to {test_case}\")\n\n    logging.info(\"Identify test case and return parameters\")\n\n    test_suite = test_suite.split(\"/\")[-1]\n\n    logging.info(f\"Return testcases for Test Suite: {test_suite}\")\n\n    suite_parameters = [\n        param for param in tests_parameters[\"test_suites\"] if param[\"name\"] == test_suite\n    ]\n\n    logging.debug(f\"Suite_parameters: {suite_parameters}\")\n\n    logging.info(f\"Return parameters for Test Case: {test_case}\")\n\n    case_parameters = [\n        param for param in suite_parameters[0][\"testcases\"] if param[\"name\"] == test_case\n    ]\n\n    logging.debug(f\"Case_parameters: {case_parameters[0]}\")\n\n    case_parameters[0][\"test_suite\"] = test_suite\n\n    return case_parameters[0]\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.import_config","title":"<code>import_config</code>","text":"<p>Check for setup file.  If setup file exists import configuration for reporting</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>Path to test case directory</p> required <code>test_suite</code> <code>dict</code> <p>Test case definition parameters</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def import_config(dir_path, test_suite):\n    \"\"\"Check for setup file.  If setup file exists import configuration for reporting\n\n    Args:\n        dir_path (str): Path to test case directory\n        test_suite (dict): Test case definition parameters\n    \"\"\"\n\n    for testcase in test_suite[\"testcases\"]:\n        if \"test_setup\" in testcase:\n            setup_file = f\"{dir_path}/{testcase['test_setup']}\"\n            logging.info(\n                f\"Importing setup file: {setup_file} into test case: {testcase['name']} definition\"\n            )\n\n            setup_config = import_yaml(setup_file)\n            logging.debug(f\"Configuration setup is {setup_config}\")\n\n            dev_ids = setup_config.get(\"key\", \"name\")\n            logging.debug(f\"Imported configuration will uses {dev_ids}\")\n\n            if dev_ids == \"name\":\n                import_config_from_name(setup_config, testcase)\n            elif dev_ids == \"role\":\n                import_config_from_role(setup_config, testcase)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.import_config_from_name","title":"<code>import_config_from_name</code>","text":"<p>Import configuration from a device name</p> <p>Parameters:</p> Name Type Description Default <code>setup_config</code> <code>dict</code> <p>Setup file data structure</p> required <code>testcase</code> <code>dict</code> <p>test case defintions data structure</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def import_config_from_name(setup_config, testcase):\n    \"\"\"Import configuration from a device name\n\n    Args:\n        setup_config (dict): Setup file data structure\n        testcase (dict): test case defintions data structure\n    \"\"\"\n\n    testcase[\"configuration\"] = \"\"\n    for dev_name in setup_config:\n        if dev_name == \"key\":\n            continue\n        testcase[\"configuration\"] += f\"{dev_name}:\\n\"\n        setup_schema = setup_config[dev_name][\"schema\"]\n\n        if setup_schema is None:\n            testcase[\"configuration\"] += f\"{setup_config[dev_name]['template']}\\n\"\n        else:\n            setup_template = Template(setup_config[dev_name][\"template\"])\n            formatted_config = setup_template.render(setup_schema)\n            testcase[\"configuration\"] += f\"{formatted_config}\\n\"\n\n        logging.debug(f\"Updated test case data structure with setup: {testcase['configuration']}\")\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.import_config_from_role","title":"<code>import_config_from_role</code>","text":"<p>Import configuration from a device role</p> <p>Parameters:</p> Name Type Description Default <code>setup_config</code> <code>dict</code> <p>Setup file data structure</p> required <code>testcase</code> <code>dict</code> <p>test case defintions data structure</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def import_config_from_role(setup_config, testcase):\n    \"\"\"Import configuration from a device role\n\n    Args:\n        setup_config (dict): Setup file data structure\n        testcase (dict): test case defintions data structure\n    \"\"\"\n\n    testcase[\"configuration\"] = \"\"\n    for role_name in setup_config:\n        if role_name != \"key\":\n            logging.debug(f\"Setting role to: {role_name}\")\n            dev_names = return_duts_with_role(role_name)\n\n            for dev_name in dev_names:\n                testcase[\"configuration\"] += f\"{dev_name}:\\n\"\n                setup_schema = setup_config[role_name][\"schema\"]\n\n                if setup_schema is None:\n                    testcase[\"configuration\"] += f\"{setup_config[role_name]['template']}\\n\"\n                else:\n                    setup_template = Template(setup_config[role_name][\"template\"])\n                    formatted_config = setup_template.render(setup_schema)\n                    testcase[\"configuration\"] += f\"{formatted_config}\\n\"\n\n                logging.debug(\n                    f\"Updated test case data structure with setup: {testcase['configuration']}\"\n                )\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.import_yaml","title":"<code>import_yaml</code>","text":"<p>Import YAML file as python data structure</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>str</code> <p>Name of YAML file</p> required <p>Returns:</p> Name Type Description <code>yaml_data</code> <code>dict</code> <p>Dictionary containing yaml data</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def import_yaml(yaml_file):\n    \"\"\"Import YAML file as python data structure\n\n    Args:\n        yaml_file (str): Name of YAML file\n\n    Returns:\n        yaml_data (dict): Dictionary containing yaml data\n    \"\"\"\n    logging.info(f\"Opening {yaml_file} for read\")\n\n    try:\n        yaml_data = yaml_read(yaml_file)\n        if yaml_data is None:\n            yaml_data = {}\n        return yaml_data\n    except OSError as err:\n        print(f\"&gt;&gt;&gt; {yaml_file} YAML FILE MISSING\")\n        logging.error(f\"ERROR YAML FILE: {yaml_file} NOT \" + f\"FOUND. {err}\")\n        logging.error(\"EXITING TEST RUNNER\")\n        sys.exit(1)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.init_duts","title":"<code>init_duts</code>","text":"<p>Use PS LLD spreadsheet to find interesting duts and then execute inputted show commands on each dut.  Return structured data of dut's output data, hostname, and connection.  Using threading to make method more efficient.</p> <p>Parameters:</p> Name Type Description Default <code>show_cmds</code> <code>str</code> <p>list of interesting show commands</p> required <code>test_parameters</code> <code>dict</code> <p>Abstraction of testing parameters</p> required <code>test_duts</code> <code>dict</code> <p>Dictionary of duts</p> required <p>Returns:</p> Name Type Description <code>duts</code> <code>dict</code> <p>structured data of duts output data, hostname, and            connection</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def init_duts(show_cmds, test_parameters, test_duts):\n    \"\"\"Use PS LLD spreadsheet to find interesting duts and then execute\n    inputted show commands on each dut.  Return structured data of\n    dut's output data, hostname, and connection.  Using threading to\n    make method more efficient.\n\n    Args:\n      show_cmds (str): list of interesting show commands\n      test_parameters (dict): Abstraction of testing parameters\n      test_duts (dict): Dictionary of duts\n\n    Returns:\n      duts (dict): structured data of duts output data, hostname, and\n                   connection\n    \"\"\"\n    logging.info(\n        \"Find DUTs and then execute inputted show commands \"\n        \"on each dut. Return structured data of DUTs output \"\n        \"data, hostname, and connection.\"\n    )\n\n    reachability, reachable_duts, unreachable_duts = check_duts_reachability(test_duts)\n\n    try:\n        continue_when_unreachable = test_parameters[\"parameters\"][\"continue_when_unreachable\"]\n    except KeyError:\n        continue_when_unreachable = False\n\n    if not (reachability or continue_when_unreachable):\n        logging.error(\n            f\"Error connecting to {unreachable_duts}, not reachable via ping, hence exiting Vane\"\n        )\n        unreachable_ips = [unreachable_dut[\"mgmt_ip\"] for unreachable_dut in unreachable_duts]\n        print(\n            \"\\x1b[31mVane encountered an error while attempting to connect to DUT/s with ip's:\\n\"\n            f\"{unreachable_ips}\\n\"\n            \"For detailed information, please refer to the logs.\\nDue to this issue, \"\n            \"Vane is exiting. \\x1b[0m\"\n        )\n        sys.exit(1)\n\n    reachable_duts, additional_unreachable_duts = login_duts(test_parameters, reachable_duts)\n    unreachable_duts.extend(additional_unreachable_duts)\n    workers = len(reachable_duts)\n\n    if not workers:\n        print(\n            \"\\x1b[31mNo valid DUTs to run tests on, hence exiting Vane.\\n\"\n            \"If you are running on/via a CVP instance ensure your DUTs are\"\n            \" not in the undefined container.\\n\"\n            \"Look at the logs for further details. \\x1b[0m\"\n        )\n        sys.exit(1)\n\n    logging.debug(f\"Duts login info: {reachable_duts} and create {workers} workers\")\n    logging.debug(f\"Passing the following show commands to workers: {show_cmds}\")\n\n    logging.info(\"Starting the execution of show commands for Vane cache\")\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        future_object = {\n            executor.submit(dut_worker, dut, show_cmds, reachable_duts): dut\n            for dut in reachable_duts\n        }\n\n    if future_object:\n        logging.debug(\"Future object generated successfully\")\n\n    logging.info(\"Returning duts data structure\")\n    logging.debug(f\"Return duts data structure: {reachable_duts}\")\n    logging.debug(f\"Return unreachable duts data structure: {unreachable_duts}\")\n\n    return reachable_duts, unreachable_duts\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.login_duts","title":"<code>login_duts</code>","text":"<p>Use eapi to connect to Arista switches for testing</p> <p>Parameters:</p> Name Type Description Default <code>test_parameters</code> <code>dict</code> <p>Abstraction of testing parameters</p> required <code>duts</code> <code>dict</code> <p>Dictionary of duts</p> required <p>Returns:</p> Name Type Description <code>reachable_duts</code> <code>list</code> <p>List of dictionaries representing dut objects             which are reachable</p> <code>unreachable_duts</code> <code>list</code> <p>List of dictionaries representing dut objects             which are unreachable (due to bad authentication)</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def login_duts(test_parameters, duts):\n    \"\"\"Use eapi to connect to Arista switches for testing\n\n    Args:\n      test_parameters (dict): Abstraction of testing parameters\n      duts (dict): Dictionary of duts\n\n    Returns:\n      reachable_duts (list): List of dictionaries representing dut objects\n                    which are reachable\n      unreachable_duts (list): List of dictionaries representing dut objects\n                    which are unreachable (due to bad authentication)\n    \"\"\"\n    logging.info(\"Using eapi/ssh to connect to Arista switches for testing\")\n\n    reachable_duts = []\n    unreachable_duts = []\n\n    network_configs = {}\n    if \"network_configs\" in test_parameters[\"parameters\"]:\n        if test_parameters[\"parameters\"][\"network_configs\"]:\n            network_configs = import_yaml(test_parameters[\"parameters\"][\"network_configs\"])\n\n    for dut in duts:\n        name = dut[\"name\"]\n        login_index = len(reachable_duts)\n        reachable_duts.append({})\n        login_ptr = reachable_duts[login_index]\n\n        logging.info(f\"Connecting to switch: {name}\")\n\n        logging.debug(f\"Connecting to switch: {name} using parameters: {dut}\")\n\n        eos_conn = test_parameters[\"parameters\"].get(\"eos_conn\", DEFAULT_EOS_CONN)\n\n        if eos_conn == \"eapi\":\n            pyeapi_conn = device_interface.PyeapiConn()\n            login_ptr[\"eapi_conn\"] = pyeapi_conn\n            login_ptr[\"connection\"] = pyeapi_conn\n        elif eos_conn == \"ssh\":\n            netmiko_conn = device_interface.NetmikoConn()\n            login_ptr[\"ssh_conn\"] = netmiko_conn\n            login_ptr[\"connection\"] = netmiko_conn\n        else:\n            raise ValueError(f\"Invalid EOS conn type {eos_conn} specified\")\n\n        success = authenticate_and_setup_conn(dut, login_ptr[\"connection\"])\n        if not success:\n            reachable_duts.pop(login_index)\n            unreachable_duts.append(dut)\n            continue\n\n        login_ptr[\"name\"] = name\n        login_ptr[\"mgmt_ip\"] = dut[\"mgmt_ip\"]\n        login_ptr[\"username\"] = dut[\"username\"]\n        login_ptr[\"password\"] = dut[\"password\"]\n        login_ptr[\"neighbors\"] = dut.get(\"neighbors\", \"\")\n        login_ptr[\"role\"] = dut.get(\"role\", \"\")\n        login_ptr[\"transport\"] = dut[\"transport\"]\n        login_ptr[\"results_dir\"] = test_parameters[\"parameters\"][\"results_dir\"]\n        login_ptr[\"report_dir\"] = test_parameters[\"parameters\"][\"report_dir\"]\n\n        if name in network_configs:\n            login_ptr[\"network_configs\"] = network_configs[name]\n\n    logging.info(f\"Returning reachable_duts: {reachable_duts}\")\n\n    return reachable_duts, unreachable_duts\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.parametrize_duts","title":"<code>parametrize_duts</code>","text":"<p>Use a filter to create input variables for PyTest parametrize</p> <p>Parameters:</p> Name Type Description Default <code>test_fname</code> <code>str</code> <p>Test suite path and file name</p> required <code>test_defs</code> <code>dict</code> <p>Dictionary with global test definitions</p> required <code>dut_objs</code> <code>dict</code> <p>Full global dictionary duts dictionary</p> required <p>Returns:</p> Name Type Description <code>dut_parameters</code> <code>dict</code> <p>Dictionary with variables PyTest parametrize for each test case.</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def parametrize_duts(test_fname, test_defs, dut_objs):\n    \"\"\"Use a filter to create input variables for PyTest parametrize\n\n    Args:\n        test_fname (str): Test suite path and file name\n        test_defs (dict): Dictionary with global test definitions\n        dut_objs (dict): Full global dictionary duts dictionary\n\n    Returns:\n        dut_parameters (dict): Dictionary with variables PyTest parametrize for each test case.\n    \"\"\"\n    logging.info(\"Discover test suite name\")\n\n    testsuite = test_fname.split(\"/\")[-1]\n\n    logging.info(f\"Filtering test definitions by test suite name: {testsuite}\")\n\n    subset_def = [defs for defs in test_defs[\"test_suites\"] if testsuite == defs[\"name\"]]\n    testcases = subset_def[0][\"testcases\"]\n\n    logging.info(\"Unpack testcases by defining dut and criteria\")\n\n    dut_parameters = {}\n\n    for testcase in testcases:\n        if \"name\" in testcase:\n            testname = testcase[\"name\"]\n            criteria = \"\"\n            dut_filter = \"\"\n\n            if \"criteria\" in testcase:\n                criteria = testcase[\"criteria\"]\n            if \"filter\" in testcase:\n                dut_filter = testcase[\"filter\"]\n\n            duts, ids = filter_duts(dut_objs, criteria, dut_filter)\n\n            logging.debug(f\"Creating dut parameters.  \\nDuts: {duts} \\nIds: {ids}\")\n\n            dut_parameters[testname] = {}\n            dut_parameters[testname][\"duts\"] = duts\n            dut_parameters[testname][\"ids\"] = ids\n\n    return dut_parameters\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.post_process_skip","title":"<code>post_process_skip</code>","text":"<p>Post processing for test case that encounters a PyTest Skip Args:     tops (obj): Test case object     steps (func): Test case     output (str): Test case show output</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def post_process_skip(tops, steps, output=\"\"):\n    \"\"\"Post processing for test case that encounters a PyTest Skip\n    Args:\n        tops (obj): Test case object\n        steps (func): Test case\n        output (str): Test case show output\n    \"\"\"\n\n    tops.skip = True\n    tops.parse_test_steps(steps)\n    tops.generate_report(tops.dut_name, output)\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.remove_cmd","title":"<code>remove_cmd</code>","text":"<p>Remove command that is not supported by pyeapi</p> <p>Parameters:</p> Name Type Description Default <code>err</code> <code>str</code> <p>Error string</p> required <code>show_cmds</code> <code>list</code> <p>List of pre-processed commands</p> required <p>Returns:</p> Name Type Description <code>show_cmds</code> <code>list</code> <p>List of post-processed commands</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def remove_cmd(err, show_cmds):\n    \"\"\"Remove command that is not supported by pyeapi\n\n    Args:\n        err (str): Error string\n        show_cmds (list): List of pre-processed commands\n\n    Returns:\n        show_cmds (list): List of post-processed commands\n    \"\"\"\n    logging.debug(f\"remove_cmd: {err}\")\n    logging.debug(f\"remove_cmd show_cmds list: {show_cmds}\")\n\n    longest_matching_cmd = \"\"\n\n    for show_cmd in show_cmds:\n        if show_cmd in str(err) and longest_matching_cmd in show_cmd:\n            longest_matching_cmd = show_cmd\n\n    # longest_matching_cmd is the one in error string, lets bump it out\n    if longest_matching_cmd:\n        cmd_index = show_cmds.index(longest_matching_cmd)\n        show_cmds.pop(cmd_index)\n\n        logging.info(f\"Removed {longest_matching_cmd} due to an error\")\n        logging.debug(f\"Removed {longest_matching_cmd} because of {err}\")\n\n    return show_cmds\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.return_duts_with_role","title":"<code>return_duts_with_role</code>","text":"<p>Create a list of duts with a role</p> <p>Parameters:</p> Name Type Description Default <code>role_name</code> <code>str</code> <p>Role to match in duts data structure</p> required <p>Returns:</p> Name Type Description <code>dev_names</code> <code>list</code> <p>Hostnames of DUTs with role</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def return_duts_with_role(role_name):\n    \"\"\"Create a list of duts with a role\n\n    Args:\n        role_name (str): Role to match in duts data structure\n\n    Returns:\n        dev_names (list): Hostnames of DUTs with role\n    \"\"\"\n\n    dev_names = []\n    for dut in config.test_duts[\"duts\"]:\n        dut_role = dut.get(\"role\", \"\")\n        if dut_role == role_name:\n            dev_names.append(dut[\"name\"])\n\n    logging.debug(f\"The following DUTs: {dev_names} have role: {role_name}\")\n\n    return dev_names\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.return_interfaces","title":"<code>return_interfaces</code>","text":"<p>Parse reachable_duts for interface connections and return them to test</p> <p>Parameters:</p> Name Type Description Default <code>hostname</code> <code>str</code> <p>hostname of dut</p> required <code>reachable_duts</code> <code>dict</code> <p>Abstraction of reachable_duts</p> required <p>Returns:</p> Name Type Description <code>interface_list</code> <code>list</code> <p>list of interesting interfaces based on                      PS LLD spreadsheet</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def return_interfaces(hostname, reachable_duts):\n    \"\"\"Parse reachable_duts for interface connections and return them to test\n\n    Args:\n        hostname (str):  hostname of dut\n        reachable_duts (dict): Abstraction of reachable_duts\n\n    Returns:\n      interface_list (list): list of interesting interfaces based on\n                             PS LLD spreadsheet\n    \"\"\"\n    logging.info(\"Parse reachable_duts for interface connections and return them to test\")\n\n    interface_list = []\n    duts = reachable_duts\n\n    for dut in duts:\n        dut_name = dut[\"name\"]\n\n        if dut_name == hostname:\n            logging.info(f\"Discovering interface parameters for: {hostname}\")\n\n            neighbors = dut.get(\"neighbors\", \"\")\n\n            for neighbor in neighbors:\n                interface = {}\n\n                logging.debug(f\"Adding interface parameters: {neighbor} neighbor for: {dut_name}\")\n\n                interface[\"hostname\"] = dut_name\n                interface[\"interface_name\"] = neighbor[\"port\"]\n                interface[\"z_hostname\"] = neighbor[\"neighborDevice\"]\n                interface[\"z_interface_name\"] = neighbor[\"neighborPort\"]\n                interface[\"media_type\"] = \"\"\n                interface_list.append(interface)\n\n    logging.info(\"Returning interface list.\")\n    logging.debug(f\"Returning interface list: {interface_list}\")\n\n    return interface_list\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.return_show_cmds","title":"<code>return_show_cmds</code>","text":"<p>Return show commands from the test_definitions</p> <p>Parameters:</p> Name Type Description Default <code>test_parameters</code> <code>dict</code> <p>Abstraction of testing parameters</p> required <p>Returns:</p> Name Type Description <code>show_cmds</code> <code>list</code> <p>show commands from the test_definitions</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def return_show_cmds(test_parameters):\n    \"\"\"Return show commands from the test_definitions\n\n    Args:\n        test_parameters (dict): Abstraction of testing parameters\n\n    Returns:\n        show_cmds (list): show commands from the test_definitions\n    \"\"\"\n    try:\n        show_clock_flag = config.test_parameters[\"parameters\"][\"show_clock\"]\n    except KeyError:\n        show_clock_flag = False\n\n    show_cmds = [\"show version\"]\n\n    if show_clock_flag:\n        show_cmds.append(\"show clock\")\n\n    logging.debug(f\"Discover the names of test suites from {test_parameters}\")\n\n    test_data = test_parameters[\"test_suites\"]\n    test_suites = [param[\"name\"] for param in test_data]\n\n    for test_suite in test_suites:\n        test_index = test_suites.index(test_suite)\n        test_cases = test_data[test_index][\"testcases\"]\n\n        logging.info(f\"Finding show commands in test suite: {test_suite}\")\n\n        for test_case in test_cases:\n            show_cmd = test_case.get(\"show_cmd\", \"\")\n            if show_cmd:\n                logging.debug(f\"Found show command {show_cmd}\")\n\n                if show_cmd not in show_cmds:\n                    logging.debug(f\"Adding Show command {show_cmd}\")\n\n                    show_cmds.append(show_cmd)\n            else:\n                test_show_cmds = test_case.get(\"show_cmds\", [])\n                logging.debug(f\"Found show commands {test_show_cmds}\")\n\n                for show_cmd in (\n                    show_cmd for show_cmd in test_show_cmds if show_cmd not in show_cmds\n                ):\n                    logging.debug(f\"Adding Show commands {show_cmd}\")\n\n                    show_cmds.append(show_cmd)\n\n    logging.info(f\"The following show commands are required for test cases: {show_cmds}\")\n\n    return show_cmds\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.return_test_defs","title":"<code>return_test_defs</code>","text":"<p>Return test_definitions from the test_parameters</p> <p>Parameters:</p> Name Type Description Default <code>test_parameters</code> <code>dict</code> <p>Abstraction of testing parameters</p> required <p>Returns:</p> Name Type Description <code>test_defs</code> <code>dict</code> <p>test definitions</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def return_test_defs(test_parameters):\n    \"\"\"Return test_definitions from the test_parameters\n\n    Args:\n        test_parameters (dict): Abstraction of testing parameters\n\n    Returns:\n        test_defs (dict): test definitions\n    \"\"\"\n    test_defs = {\"test_suites\": []}\n    test_dirs = test_parameters[\"parameters\"][\"test_dirs\"]\n    report_dir = test_parameters[\"parameters\"][\"report_dir\"]\n    test_definitions_file = test_parameters[\"parameters\"][\"test_definitions\"]\n\n    for test_directory in test_dirs:\n        tests_info = os.walk(test_directory)\n        for dir_path, _, file_names in tests_info:\n            for file_name in file_names:\n                if file_name == test_definitions_file:\n                    file_path = f\"{dir_path}/{file_name}\"\n                    test_def = import_yaml(file_path)\n                    for test_suite in test_def:\n                        test_suite[\"dir_path\"] = f\"{dir_path}\"\n                        import_config(dir_path, test_suite)\n                    test_defs[\"test_suites\"] += test_def\n\n    logging.info(f\"Creating {report_dir} reports directory\")\n    os.makedirs(report_dir, exist_ok=True)\n    export_yaml(report_dir + \"/\" + test_definitions_file, test_defs)\n\n    logging.debug(f\"Return the following test definitions data structure {test_defs}\")\n\n    return test_defs\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.send_cmds","title":"<code>send_cmds</code>","text":"<p>Send show commands to duts and recurse on failure</p> <p>Parameters:</p> Name Type Description Default <code>show_cmds</code> <code>list</code> <p>List of pre-processed commands</p> required <code>conn</code> <code>obj</code> <p>connection</p> required <code>encoding</code> <code>str</code> <p>encoding type of show commands: either json or text</p> required <p>Returns:</p> Name Type Description <code>show_cmd_list</code> <code>list</code> <p>list of show command outputs</p> <code>show_cmds</code> <code>list</code> <p>list of show commands</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def send_cmds(show_cmds, conn, encoding):\n    \"\"\"Send show commands to duts and recurse on failure\n\n    Args:\n        show_cmds (list): List of pre-processed commands\n        conn (obj): connection\n        encoding (str): encoding type of show commands: either json or text\n\n    Returns:\n        show_cmd_list (list): list of show command outputs\n        show_cmds (list): list of show commands\n    \"\"\"\n\n    try:\n        logging.debug(f\"List of show commands in show_cmds with encoding {encoding}: {show_cmds}\")\n\n        if encoding == \"json\":\n            show_cmd_list = conn.run_commands(show_cmds)\n        elif encoding == \"text\":\n            show_cmd_list = conn.run_commands(show_cmds, encoding=\"text\")\n\n        logging.info(f\"Ran all show commands on dut to gather {encoding} data\")\n        logging.debug(f\"Ran all show cmds with encoding {encoding}: {show_cmds}\")\n\n    # pylint: disable-next=broad-exception-caught\n    except Exception as err:\n        logging.error(f\"Error running all cmds: {err}\")\n\n        show_cmds = remove_cmd(err, show_cmds)\n\n        logging.debug(f\"New show_cmds: {show_cmds}\")\n\n        show_cmd_list = send_cmds(show_cmds, conn, encoding)\n        show_cmd_list = show_cmd_list[0]\n\n    logging.debug(f\"Return all show cmds: {show_cmd_list}\")\n\n    return show_cmd_list, show_cmds\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.verify_show_cmd","title":"<code>verify_show_cmd</code>","text":"<p>Verify if show command was successfully executed on dut</p> <p>Parameters:</p> Name Type Description Default <code>show_cmd</code> <code>str</code> <p>show command</p> required <code>dut</code> <code>dict</code> <p>data structure of dut parameters</p> required Source code in <code>vane/tests_tools.py</code> <pre><code>def verify_show_cmd(show_cmd, dut):\n    \"\"\"Verify if show command was successfully executed on dut\n\n    Args:\n        show_cmd (str): show command\n        dut (dict): data structure of dut parameters\n    \"\"\"\n\n    dut_name = dut[\"name\"]\n\n    logging.info(\n        f\"Verifying if show command {show_cmd} was successfully executed on {dut_name} dut\"\n    )\n\n    if show_cmd in dut[\"output\"]:\n        logging.debug(f\"Verified output for show command {show_cmd} on {dut_name}\")\n    else:\n        logging.critical(f\"Show command {show_cmd} not executed on {dut_name}\")\n\n        assert False\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.verify_tacacs","title":"<code>verify_tacacs</code>","text":"<p>Verify if tacacs servers are configured</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>data structure of dut parameters</p> required <p>Returns:</p> Name Type Description <code>tacacs_bool</code> <code>bool</code> <p>boolean representing if tacacs server(s) are configured or not</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def verify_tacacs(dut):\n    \"\"\"Verify if tacacs servers are configured\n\n    Args:\n        dut (dict): data structure of dut parameters\n\n    Returns:\n        tacacs_bool (bool): boolean representing if tacacs server(s) are configured or not\n    \"\"\"\n    dut_name = dut[\"name\"]\n    show_cmd = \"show tacacs\"\n    tacacs_bool = True\n    tacacs = dut[\"output\"][show_cmd][\"json\"][\"tacacsServers\"]\n    tacacs_servers = len(tacacs)\n\n    logging.info(f\"Verifying if tacacs server(s) are configured on {dut_name} dut\")\n\n    if tacacs_servers == 0:\n        tacacs_bool = False\n\n    logging.debug(f\"{tacacs_servers} tacacs servers are configured so returning {tacacs_bool}\")\n\n    return tacacs_bool\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.verify_veos","title":"<code>verify_veos</code>","text":"<p>Verify if DUT is a VEOS instance</p> <p>Parameters:</p> Name Type Description Default <code>dut</code> <code>dict</code> <p>data structure of dut parameters</p> required <p>Returns:</p> Name Type Description <code>veos_bool</code> <code>bool</code> <p>boolean representing if the dut is a VEOS instance or not.</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def verify_veos(dut):\n    \"\"\"Verify if DUT is a VEOS instance\n\n    Args:\n        dut (dict): data structure of dut parameters\n\n    Returns:\n        veos_bool (bool): boolean representing if the dut is a VEOS instance or not.\n    \"\"\"\n    dut_name = dut[\"name\"]\n    show_cmd = \"show version\"\n    veos_bool = False\n    veos = dut[\"output\"][show_cmd][\"json\"][\"modelName\"]\n\n    logging.info(f\"Verifying if {dut_name} DUT is a VEOS instance. Model is {veos}\")\n\n    if \"vEOS\" in veos:\n        veos_bool = True\n\n        logging.debug(f\"{dut_name} is a VEOS instance so returning {veos_bool}\")\n    else:\n        logging.debug(f\"{dut_name} is not a VEOS instance so returning {veos_bool}\")\n\n    return veos_bool\n</code></pre>"},{"location":"api_cli/api.html#vane.tests_tools.yaml_read","title":"<code>yaml_read</code>","text":"<p>Return yaml data read from the yaml file</p> <p>Parameters:</p> Name Type Description Default <code>yaml_file</code> <code>file</code> <p>Input yaml file to be read</p> required <p>Returns:</p> Name Type Description <code>yaml_data</code> <code>dict</code> <p>Yaml data read from the file</p> Source code in <code>vane/tests_tools.py</code> <pre><code>def yaml_read(yaml_file):\n    \"\"\"Return yaml data read from the yaml file\n\n    Args:\n        yaml_file (file): Input yaml file to be read\n\n    Returns:\n        yaml_data (dict):Yaml data read from the file\n    \"\"\"\n    with open(yaml_file, \"r\", encoding=\"utf-8\") as input_yaml:\n        try:\n            yaml_data = yaml.safe_load(input_yaml)\n            logging.debug(f\"Inputted the following yaml: {yaml_data}\")\n            return yaml_data\n        except yaml.YAMLError as err:\n            print(\"&gt;&gt;&gt; ERROR IN YAML FILE\")\n            logging.error(f\"ERROR IN YAML FILE: {err}\")\n            logging.error(\"EXITING TEST RUNNER\")\n            sys.exit(1)\n</code></pre>"},{"location":"api_cli/cli.html","title":"Vane CLI","text":"<p>Vane has the following Command Line arguments which we will be discussing in this section</p> <p>Vane Help</p> <p></p> <p>We discussed the --definitions-file and --duts-file flag in the Executing Vane Section and --nrfu flag in the NRFU Testing Section, we will cover the rest here.</p>"},{"location":"api_cli/cli.html#using-the-generate-duts-file-flag","title":"Using the -- generate-duts-file flag","text":"<p>This flag can be used to generate the duts file with a given name (via the --duts-file-name flag) for a given inventory and topology file of a deployed network. If the --duts-file-name flag is not used, Vane defaults to using duts.yaml for the filename</p> <pre><code>vane \u2013-generate-duts-file topology.yaml inventory.yaml\n</code></pre> <p>topology.yaml : this file represents the topology of the lab you will run Vane against. The topology.yaml file abides by the following format.</p> Sample topology.yaml<pre><code>veos:\n# Define the global vEOS parameters\nusername: username\npassword: password\nversion: 4.27.2F\n\nnodes:\n# Define individual node parameters\n- DSR01:\n    ip_addr: 192.168.0.9\n    node_type: veos\n    neighbors:\n      - neighborDevice: DCBBW1\n        neighborPort: Ethernet1\n        port: Ethernet1\n      - neighborDevice: DCBBW2\n        neighborPort: Ethernet1\n        port: Ethernet2\n      - neighborDevice: DCBBE1\n        neighborPort: Ethernet1\n        port: Ethernet3\n      - neighborDevice: DCBBE2\n        neighborPort: Ethernet1\n        port: Ethernet4\n- DCBBW1:\n    ip_addr: 192.168.0.10\n    node_type: veos\n    neighbors:\n      - neighborDevice: DSR01\n        neighborPort: Ethernet1\n        port: Ethernet1\n      - neighborDevice: BLFW1\n        neighborPort: Ethernet1\n        port: Ethernet3\n      - neighborDevice: BLFW2\n        neighborPort: Ethernet1\n        port: Ethernet4\n</code></pre> <p>inventory.yaml : this file can be downloaded from ACT (if running Vane against a virtual ACT lab) from the view which reflects your deployed lab. If not using ACT, the inventory.yaml file can be created by abiding by the following format.</p> Sample inventory.yaml<pre><code>all:\n  children:\n    CVP:\n      hosts:\n        cv_ztp:\n          ansible_host: 10.255.62.114\n          ansible_user: username\n          ansible_password: password\n        cv_server:\n          ansible_httpapi_host: 10.255.62.114\n          ansible_host: 10.255.62.114\n          ansible_user: username\n          ansible_password: password!\n    VEOS:\n      hosts:\n        DCBBW1:\n          ansible_host: 10.255.31.234\n          ansible_user: username\n          ansible_ssh_pass: password!\n        DSR01:\n          ansible_host: 10.255.50.212\n          ansible_user: username\n          ansible_ssh_pass: password!\n</code></pre> <p>Success</p> <p>Once you have these two files in your vane directory, you can use this command to generate a duts file which Vane will run against.</p> <pre><code>vane \u2013-generate-duts-file topology.yaml inventory.yaml\n</code></pre>"},{"location":"api_cli/cli.html#using-the-duts-file-name-flag","title":"Using the -- duts-file-name flag","text":"<p>As mentioned above this flag is supposed to be used in tandem with the --generate-duts-file flag in order to provide a name for the generated duts file</p> <pre><code>vane \u2013-generate-duts-file topology.yaml inventory.yaml --duts-file-name generated_duts.yaml\n</code></pre> <p>This would generate a duts file by the name of generated_duts.yaml reflecting the provided topology and inventory files.</p>"},{"location":"api_cli/cli.html#using-the-generate-test-catalog-flag","title":"Using the -- generate-test-catalog flag","text":"<p>This flag can be used to generate a test catalog csv file for test directory/s passed as the argument.</p> <pre><code>vane --generate-test-catalog path/to/test/case/directory\n</code></pre> <p>This will generate a csv file that has details regarding the test suite, test case id, test case name, test case description and test case steps for each of the test case belonging to the passed in test directory. This csv file will be stamped with the time of generation and would be created within a folder called test catalog.</p> <p>Test Catalog</p> Sample .csv file <p></p> <p>Tip</p> <p>If using non-default test definitions (file named other than test_definition.yaml file), include the --test-definitions-file argument to specify the name of test definitions file relevant to the passed in test directories.</p> <p>Warning</p> <p>Vane enforces a common test definitions file name across all test directories. This implies that the --test-definitions-file flag can take in only 1 file name.</p>"},{"location":"api_cli/cli.html#using-the-test-definitions-file-flag","title":"Using the --test-definitions-file flag","text":"<p>This flag can be used in tandem with the --generate-test-catalog flag wherein you want to provide a custom named test definitions file for your test directory. By default Vane would look for a file named test_definitions.yaml within your test directory to locate the parameters associated with your test cases. You can alter this behaviour if you have your test definition data in a file named otherwise by providing the custom name via this argument.</p> <pre><code>vane --generate-test-catalog sample_network_tests/memory --test-definitions-file memory_test_definitions.yaml\n</code></pre> <p>Warning</p> <p>The argument takes in the name of the test definitions file and NOT the path</p>"},{"location":"api_cli/cli.html#using-the-markers-flag","title":"Using the -- markers flag","text":"<p>This flag can be used to view all the markers supported by Vane</p> <pre><code>vane --markers\n</code></pre> <p>Note</p> <p>Markers currently defined in the Vane distribution (newer customized ones can be added in the pytest.ini file):</p> View Markers <pre><code>[{'marker': 'filesystem', 'description': 'EOS File System Test Suite'},\n{'marker': 'daemons', 'description': 'EOS daemons Test Suite'},\n{'marker': 'extensions', 'description': 'EOS extensions Test Suite'},\n{'marker': 'users', 'description': 'EOS users Test Suite'},\n{'marker': 'tacacs', 'description': 'TACACS Test Suite'},\n{'marker': 'aaa', 'description': 'AAA Test Suite'},\n{'marker': 'host', 'description': 'Host status Test Suite'},\n{'marker': 'base_feature', 'description': 'Run all base feature test suites'},\n{'marker': 'platform_status', 'description':\n'Run all DUT platform status test suites'},\n{'marker': 'authorization', 'description':\n'Run all authorization test cases in AAA Test Suite'},\n{'marker': 'authentication', 'description':\n'Run all authentication test cases in AAA Test Suite'},\n{'marker': 'accounting', 'description':\n'Run all accounting test cases in AAA Test Suite'},\n{'marker': 'api', 'description': 'API Test Suite'},\n{'marker': 'dns', 'description': 'DNS Test Suite'},\n{'marker': 'logging', 'description': 'Logging Test Suite'},\n{'marker': 'ztp', 'description': 'Zero Touch Provisioning Test Suite'},\n{'marker': 'ntp', 'description': 'NTP Test Suite'},\n{'marker': 'nrfu', 'description': 'Network Ready For Use Test Cases'},\n{'marker': 'pytest', 'description': 'PyTest Test Suite'},\n{'marker': 'environment', 'description': 'Environment Test Suite'},\n{'marker': 'cpu', 'description': 'CPU Test Suite'},\n{'marker': 'memory', 'description': 'Memory Test Suite'},\n{'marker': 'interface', 'description': 'Interface Test Suite'},\n{'marker': 'interface_baseline_health', 'description':\n'Run all interface baseline health test suites'},\n{'marker': 'l2_protocols', 'description': 'Run all L2 protocol test suites'},\n{'marker': 'lldp', 'description': 'Memory Test Suite'},\n{'marker': 'system', 'description': 'System Test Suite'},\n{'marker': 'demo', 'description': 'Tests ready to demo'},\n{'marker': 'physical', 'description': 'Tests that can run on physical hardware'},\n{'marker': 'virtual', 'description': 'Tests that can run on vEOS'},\n{'marker': 'eos424', 'description': 'Validated tests with EOS 4.24'},\n{'marker': 'ssh', 'description': 'Verify SSH version'},\n{'marker': 'vane_system_tests', 'description': 'Verify vane functionalities'},\n{'marker': 'security', 'description': 'Test cases related to security functionalities'},\n{'marker': 'nrfu_test', 'description': 'Network Ready For Use test cases'},\n{'marker': 'routing', 'description': 'Test cases related to routing functionalities'},\n{'marker': 'misc', 'description': 'Miscellaneous test cases'},\n{'marker': 'interfaces', 'description':\n'Test cases related to interface functionalities'},\n{'marker': 'base_services', 'description': 'Test cases related to base services'},\n{'marker': 'xdist_group', 'description':\n'specify group for tests should run in same session.in relation to one another.'\n'Provided by pytest-xdist.'}]\n</code></pre> <p>In Pytest, markers are a way to add metadata or labels to your test functions. You can use markers to group tests together. For example, you might have markers like @pytest.mark.smoke or @pytest.mark.functional to categorize tests based on their purpose.</p> <pre><code>@pytest.mark.smoke\ndef test_smoke_feature():\n    # Test code goes here\n</code></pre> <p>You can now run specific groups of tests using markers. For example, if you only want to run smoke tests, you can add the smoke marker in the definitions.yaml file's marker field as follows. The details on how a marker can be specified are linked below.</p> definitions.yaml<pre><code>parameters:\n  ...\n  html_report: reports/report\n  json_report: reports/report\n  mark: smoke\n  processes: null\n  report_dir: reports\n  results_file: result.yml\n  results_dir: reports/results\n  test_cases: All\n  test_dirs: \n  - sample_network_tests\n  report_test_steps: true\n  ...\n</code></pre> <p>Markers make it easy to organize and manage your tests, especially in larger test suites where you might have various types of tests with different requirements.</p> <p>Info</p> <p>You can see the official documentation of Pytest Markers here.</p>"},{"location":"differentiation/differentiation.html","title":"Win With Vane","text":"In the traditional network operating model, network certification testing has been a barrier to network changes.         The problem is testing can take a significant amount of time which is usually measured in months.  During this time organizations are prevented from taking advantage of new platform economics like transitioning to higher speed ethernet which can reduce the number of links required in a topology, or using new silicon which creates higher port density and improves scaling.  It also prevents new features from entering the network which could simplify functionality or be applied to a service.  Lastly, it prevents a new service that could create new revenue streams for the organization from being introduced.       Using the Vane Testing Framework and Arista\u2019s expertise, network certification test cases can be automated.  This is disruptive to traditional network certification testing because it is easy to use, easy to create test cases, and fast.  <p>Below are the benefits of Vane Testing Framework:</p>"},{"location":"differentiation/differentiation.html#versatility-in-testing-scenarios","title":"Versatility in Testing Scenarios","text":"<p>Traditional and Advanced Tests: While NRFU testing focuses on basic readiness, our framework extends its capabilities to conduct a wide range of tests, including performance, security, scalability, and hardware tests. This ensures that the network is not only ready but optimized for diverse scenarios.</p>"},{"location":"differentiation/differentiation.html#real-world-network-testing","title":"Real-world Network Testing","text":"<p>Physical and Virtual Environments: Our framework breaks the barrier between virtual and physical networks. It not only lets you test your virtual network twin but also allows for testing on actual hardware, ensuring that the network's real-world performance matches expectations. This is crucial for industries where physical infrastructure plays a significant role.</p>"},{"location":"differentiation/differentiation.html#holistic-network-wide-validation","title":"Holistic Network-wide Validation","text":"<p>Beyond Single Device Testing: Our framework takes a holistic approach by communicating with and testing the entire network, not just individual devices. This ensures that the network as a whole is thoroughly validated, identifying potential bottlenecks, inter-device dependencies, and overall system efficiency.</p>"},{"location":"differentiation/differentiation.html#ease-of-integration-with-existing-workflows","title":"Ease of Integration with Existing Workflows","text":"<p>Customization and Adaptability: By supporting configurations ingested from AVD or YAML files, Vane aligns with existing development and operational workflows. This ensures a smooth integration process for organizations that have established practices using specific tools or file formats. The tool becomes an integral part of the existing ecosystem, minimizing disruptions and accelerating the adoption of network validation practices.</p>"},{"location":"differentiation/differentiation.html#open-source","title":"Open Source","text":"<p>The Vane Testing Framework is open source and available on Arista Network\u2019s public GitHub repository. The repository includes the Vane application and sample test cases.  </p> <p>Open source software is free to use, reducing costs for organizations to adapt.  Users have the freedom to modify and adapt the software to meet their specific needs.  With many eyes on the code, vulnerabilities can be identified and fixed quickly, enhancing the software's security.  Open source projects have strong communities that offer support, contribute to development, and share knowledge.  Open source fosters innovation as developers from around the world can contribute new ideas and improvements.</p>"},{"location":"differentiation/differentiation.html#agnostic-to-infrastructure","title":"Agnostic to Infrastructure","text":"<p>The Vane application is designed to run on any platform including local computing like a laptop or a desktop, a server, or the cloud.  Vane can run on any Unix or Windows platform, in a Python virtual environment, or within a container.  There is also a CloudVision application available.</p> <p>The Vane application is able to qualify physical network infrastructure, virtual network infrastructure like Arista Cloud Test, or a hybrid of virtual and physical.  Using virtual infrastructure can greatly reduce the cost of testing and increase test coverage.  However, some tests will require physical hardware if the virtual infrastructure cannot be replicated in software like Quality of Service or Multicast forwarding tests.  It should be noted Vane is unable to replace tests which require human interaction like removing components within an Arista switch.</p>"},{"location":"executing_vane/executing_vane.html","title":"Executing Vane","text":"<p>The application is run through the command line by passing in different arguments as needed. Using the command <code>vane --help</code> shows the different available arguments that can be used.</p> <pre><code>(vane-env) vane # vane --help\n</code></pre> <p></p> <p>Note</p> <p>Let us get started by discussing some of the necessary flags/arguments, namely definitionas and duts file, what they stand for and how they can be included in order to help you get started with Vane. The remaining arguments and api will be discussed in the API and CLI Referenec Section.</p>"},{"location":"executing_vane/executing_vane.html#using-the-definitions-file-flag","title":"Using the definitions file flag","text":"<pre><code>--definitions_file definitions.yaml\n</code></pre> <p>The Vane definitions file serves as an informative file to Vane, it includes different information, key fields are highlighted below:</p> <ul> <li>test_dirs: the source directory which contains the test cases to be run, in our   sample definitions file we pass the sample_network_tests as the test directory.   Sample Network Tests   is a directory of tests provided by us which tests some basics of a network</li> <li>report_dir: the directory where report files generated by Vane should be stored,</li> <li>test_cases:  list of test cases within the test_dir which need to be run,</li> </ul> <p>Variables in this file can be edited by the operator to cater to the application execution.</p> <p>You can provide the definitions file while running Vane as follows:</p> <pre><code>vane \u2013definitions_file definitions.yaml\n</code></pre> <p>Note</p> <p>definitions.yaml is the relative path to your definitions file, the sample one in vane repo is at vane/sample_network_tests/definitions.yaml</p> <p>If you do not include this argument then vane defaults to using the path mentioned in the vane/config.py file</p> Sample definitions.yaml<pre><code># This is a sample definitions.yaml file which can be used to run vane.\n# The fields below can be changed to reflect specific test cases,\n# test directories which need to be run.\n\nparameters:\n  eapi_file: tests/unittests/fixtures/eapi.conf\n  eapi_template: tests/fixtures/templates/eapi.conf.j2\n  eos_conn: eapi\n  excel_report: null\n  html_report: reports/report\n  json_report: reports/report\n  mark: \n  processes: null\n  report_dir: reports\n  results_file: result.yml\n  results_dir: reports/results\n  setup_show: false\n  show_log: show_output.log\n  stdout: false\n  test_cases: All\n  test_dirs: \n  - sample_network_tests\n  report_test_steps: true\n  generate_test_definitions: false\n  master_definitions: master_def.yaml\n  template_definitions: tests_definitions.yaml\n  test_definitions: test_definition.yaml\n  verbose: true\n  spreadsheet: tests/fixtures/spreadsheets/PS-LLD-Questionnaire-Template.xlsx\n  xcel_definitions: tests/fixtures/spreadsheets/xcel_definitions.yaml\n  xcel_schema: tests/fixtures/spreadsheets/xcel_schema.yaml\n</code></pre>"},{"location":"executing_vane/executing_vane.html#using-the-duts-file-flag","title":"Using the duts file flag","text":"<pre><code>\u2013duts_file duts.yml\n</code></pre> <p>The Vane duts file includes a list of all devices that Vane should run its test cases against, it includes relevant information for each DUT (device under test) such as their hostname and their access credentials. Operators should edit this file to include the devices that they would like to run Vane against.</p> <p>Note</p> <p>In the CLI section later on we talk about how we can use certain flag to generate this duts.yaml file instead of having to manually fill in all the details</p> <p>You can provide the duts file while running Vane as follows:</p> <pre><code>vane \u2013duts-file  duts.yaml\n</code></pre> <p>Note</p> <p>duts.yaml is the relative path to your duts file, you can use the sample one in vane repo located at vane/sample_network_tests/duts.yaml to get started.</p> <p>If you do not include this argument then vane defaults to using the path mentioned in the vane/config.py file</p> Sample duts.yaml<pre><code># This is a sample duts.yaml file which can be used to run vane.\n# Ensure you edit the ip addresses to reflect your DUTS\n\nduts:\n- mgmt_ip: 10.255.106.71\n  name: DSR01\n  neighbors:\n  - neighborDevice: DCBBW1\n    neighborPort: Ethernet1\n    port: Ethernet1\n  - neighborDevice: DCBBW2\n    neighborPort: Ethernet1\n    port: Ethernet2\n  - neighborDevice: DCBBE1\n    neighborPort: Ethernet1\n    port: Ethernet3\n  - neighborDevice: DCBBE2\n    neighborPort: Ethernet1\n    port: Ethernet4\n  password: cvp123!\n  transport: https\n  username: cvpadmin\n  role: unknown\n- mgmt_ip: 10.255.70.133\n  name: DCBBW1\n  neighbors:\n  - neighborDevice: DSR01\n    neighborPort: Ethernet1\n    port: Ethernet1\n  password: cvp123!\n  transport: https\n  username: cvpadmin\n  role: unknown\n</code></pre> <p>There are a few other flags with descriptions on how to use them which you can explore further in the CLI section, but the ones described above are the necessary ones to get started.</p>"},{"location":"executing_vane/executing_vane.html#running-vane","title":"Running Vane","text":"<p>Command to Run Vane:</p> <pre><code>vane\n</code></pre> <p>Note</p> <p>Ensure the DEFINITIONS_FILE and DUTS_FILE variables in config.py are pointing to the correct location for the respective files if you want to run vane using \u201cvane\u201d command, if not you might have to explicitly mention the arguments and give the locations in the command as follows:</p> <pre><code>vane \u2013definitions-file sample_network_tests/definitions.yaml \u2013duts-file sample_network_tests/duts.yaml\n</code></pre> <p>Below is a sample run of vane:</p> <p></p> <p>Success</p> <p>If the test cases get executed correctly, it implies vane has been set up correctly, the failure of test cases in itself does not imply an error on vane\u2019s execution side of things.</p>"},{"location":"executing_vane/executing_vane.html#viewing-reports-generated-by-vane","title":"Viewing Reports generated by Vane","text":"<p>After Vane has executed successfully, test case reports get generated and populated in the reports folder which exist in the outermost directory of vane. You can view these reports in multiple formats, including json, .docx, html. These reports offer detailed information on the test cases such as test case procedure, input, expected output, pass/fail result, and other relevant observations.</p> Sample Reports Sample .docx ReportSample .html Report (Details hidden)Sample .html Report (Details shown) <p> </p> <p></p> <p> </p> <p>Vane Logs</p> <p>You can also gather more insights into your test execution by accessing the logs folder located in the root of your project directory. This folder would have a vane.log file with overall logs and additionally also have individual test case logs.</p> <p>Warning</p> <p>The logs folder gets cleaned at the beginning of each vane run to delete test case logs (plus any additional files you may have created in it) and only maintains the vane.log file.</p>"},{"location":"gui/user_interface.html","title":"Coming Soon","text":""},{"location":"installation/docker_installation.html","title":"Via Docker","text":"<p>Vane can be installed by creating and running a docker container. The DockerFile has been provided in the cloned repo</p> <p>The following steps should get you started</p>"},{"location":"installation/docker_installation.html#clone-the-vane-repository","title":"Clone the Vane Repository","text":"<pre><code>git clone https://github.com/aristanetworks/vane.git\n</code></pre>"},{"location":"installation/docker_installation.html#enter-the-project-root-folder-and-build-the-docker-container","title":"Enter the Project Root Folder and Build the docker container","text":"<p>This step will ensure it downloads all the dependencies needed to run Vane in the isolated docker environment, for example it will install poetry and create a virtual environment with the required dependencies.</p> <p>Warning</p> <p>Ensure your docker daemon is running before building the container otherwise the make container command will error out.</p> <pre><code>cd vane\nmake container\n</code></pre>"},{"location":"installation/docker_installation.html#run-the-container","title":"Run the Container","text":"<pre><code>make run\n</code></pre> <p>After this command you will see the following screen</p> <p></p>"},{"location":"installation/docker_installation.html#activate-the-virtual-environment","title":"Activate the virtual environment","text":"<p>Warning</p> <p>Make sure you hit activate before trying the Vane commands since you need to enter the Vane poetry virtual environment with the installed dependencies before Vane commands can be recognized.</p> <pre><code>activate\n</code></pre> Note <p>The Vane project repo is mounted in the docker container as /project directory. In order to modify source code or refer to any files which exist within the vane repo use the mounted \"project\" folder instead. For eg, the sample_network_tests folder which exists at vane/sample_network_tests in the repo can now be found at /project/sample_network_tests. Changes to this folder are saved across containers</p> Warning <p>In case by default you do not get dropped in the /project dir, then explicitly enter the project folder by executing the following command. This is important because the pytest.ini file exists in the /project folder and Vane needs that file in the root folder to execute test cases.</p> <pre><code>cd /project\n</code></pre> <p>Vane is now ready to be executed and the prompt will look as follows:</p> <pre><code>(vane-dev-shell) vane-container project #\n\n(vane-dev-shell) vane-container project # vane --help\n</code></pre> <p></p> <p>To exit out of the container execute the following command:</p> <pre><code>exit\n</code></pre> <p>Success</p> <p>Now that you are all set up, navigate to the Executing Vane Section to learn about how to use Vane and its different commands to execute test cases on your network.</p>"},{"location":"installation/installation_home.html","title":"Installation","text":"<p>Welcome to the installation guide for Vane. Choose the installation method that best suits your needs:</p>"},{"location":"installation/installation_home.html#1-installation-via-python-virtual-environment","title":"1. Installation via Python Virtual Environment","text":"<p>Using Python virtual environments for application installation offers critical advantages in managing project dependencies and maintaining consistency across development workflows. Virtual environments allow for the isolation of project-specific dependencies, preventing conflicts between different projects and ensure that each project has access to the precise versions of libraries it requires.</p> <p>This is the best option for most users and only requires Python to implement. For those who prefer using Python virtual environments, we have a detailed guide to help you set up your project environment using the traditional Python virtual environment approach.</p> <p>Explore the Python Virtual Environment installation method</p>"},{"location":"installation/installation_home.html#2-installation-via-docker","title":"2. Installation via Docker","text":"<p>Docker has many of same benefits as a Python Virtual Environment. Docker simplifies project management by encapsulating applications and dependencies in containers, ensuring consistency across diverse environments. Its lightweight nature enables efficient resource usage, while the ability to version and share Docker images facilitates collaboration and reproducibility. The DockerFile for Vane has been provided in the cloned repo.</p> <p>However, Docker requires a Docker engine (like Docker Desktop) to implement. A Docker installations is best for integration into continuous integration and deployment pipelines, enhancing development workflows and productivity.</p> <p>Simplify the installation process by using Docker containers. Our Docker guide provides instructions on how to quickly get started with our project in a containerized environment.</p> <p>Get started with Docker installation</p>"},{"location":"installation/installation_home.html#3-installation-via-poetry","title":"3. Installation via Poetry","text":"<p>Poetry also has many of same benefits as a Python Virtual Environment. Its straightforward configuration, comprehensive dependency resolution, and lock file system ensure consistent environments across different machines. Poetry streamlines the development process, making it easy to declare and manage project dependencies. It is the best route for contributing to Vane development.</p> <p>However, this method requires the Python Poetry package for installation.</p> <p>If you prefer managing your project dependencies with Poetry, follow our step-by-step guide on installing and setting up your environment using Poetry.</p> <p>Learn more about installing via Poetry</p> <p>Choose the method that aligns with your workflow and dive into the respective section for detailed instructions.</p>"},{"location":"installation/poetry_installation.html","title":"Via Poetry","text":"<p>Note</p> <p>If you do not want to follow the steps below we have an installation script which automates the manual installation procedure via poetry. Follow the instructions in Installation Script for Vane to install Vane using the Installation script.</p> <p>Vane can be installed using poetry which sets up a python virtual environment by following the steps below:</p>"},{"location":"installation/poetry_installation.html#clone-the-vane-repository","title":"Clone the Vane Repository","text":"<pre><code>git clone https://github.com/aristanetworks/vane.git\n</code></pre>"},{"location":"installation/poetry_installation.html#install-poetry","title":"Install Poetry","text":"<p>Check if you already have poetry installed using the following command</p> <pre><code>poetry --version\n</code></pre> <p>Error</p> <p>If you get a command not found error, install poetry using the following command and ensure it has been installed correctly and its path has been set correctly by trying the version command again.</p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre>"},{"location":"installation/poetry_installation.html#resources","title":"Resources","text":"<ul> <li>Troubleshooting while installing poetry</li> <li>Official documentation for installing</li> </ul>"},{"location":"installation/poetry_installation.html#configuring-poetry","title":"Configuring Poetry","text":"<p>We will now configure poetry to spin up the virtual environment in the project root directory instead of its default location</p> <p>Check currently configured location by running the following command and checking the virtualenvs.path field</p> <pre><code>poetry config --list\n</code></pre> <p>We need to change this default to reflect our project root directory, enter the following command to achieve that and replace [path_to_project_root_folder] with actual path to the project root directory</p> <pre><code>poetry config virtualenvs.path [path_to_project_root_folder]\n</code></pre> <p>Verify the change has taken place by viewing the config again</p>"},{"location":"installation/poetry_installation.html#spinning-up-the-virtual-environment","title":"Spinning Up the Virtual Environment","text":"<p>Now we need to spin up the virtual environment with all the dependencies mentioned in the pyproject.toml file, enter the following command for poetry to generate a poetry.lock file and create a virtual environment in the project root folder with the needed dependencies.</p> <pre><code>poetry install\n</code></pre> <p>To enter the virtual environment run the following command</p> <pre><code>poetry shell \nor\nsource path_to_virtual-environment/bin/activate\n</code></pre> <p>In either case, the prompt will change to indicate the virtual environment is active by prefixing the project name and python version, and Vane can now be executed in the environment.</p> <p>Vane is now ready to be executed and the prompt will look as follows:</p> <pre><code>(vane-py3.9) vane #\n</code></pre> <pre><code>(vane-py3.9) vane # vane --help\n</code></pre> <p></p> <p>To exit out of the virtual environment execute the following command:</p> <pre><code>deactivate\n</code></pre> <p>Success</p> <p>Now that you are all set up, navigate to the Executing Vane Section to learn about how to use Vane and its different commands to execute test cases on your network.</p>"},{"location":"installation/python_installation.html","title":"Via Python Virtual Environment","text":"<p>Vane can be installed using venv which is a module in Python's standard library (as of Python 3.3) that provides support for creating lightweight, isolated Python environments.</p> <p>The following steps should get you started:</p>"},{"location":"installation/python_installation.html#clone-the-vane-repository","title":"Clone the Vane Repository","text":"<pre><code>git clone https://github.com/aristanetworks/vane.git\n</code></pre>"},{"location":"installation/python_installation.html#enter-the-project-root-directory-and-create-a-virtual-environment","title":"Enter the Project Root Directory and Create a Virtual environment","text":"<pre><code>cd vane\npython3 -m venv venv\n</code></pre>"},{"location":"installation/python_installation.html#activate-the-virtual-environment","title":"Activate the Virtual Environment","text":"<pre><code>source venv/bin/activate\n</code></pre>"},{"location":"installation/python_installation.html#install-the-requirements","title":"Install the requirements","text":"<pre><code>pip install .\n</code></pre> <p>Warning</p> <p>You might have to exit and enter the venv again for the installation changes to reflect. Additionally, for any source code change to reflect in the virtual environment, the above command needs to be issued every time after the change.</p> <p>Vane is now ready to be executed and the prompt will look as follows:</p> <pre><code>(venv) vane #\n</code></pre> <pre><code>(venv) vane # vane --help\n</code></pre> <p></p> <p>To exit out of the virtual environment execute the following command:</p> <pre><code>exit\n</code></pre> <p>Success</p> <p>Now that you are all set up, navigate to the Executing Vane Section to learn about how to use Vane and its different commands to execute test cases on your network.</p>"},{"location":"nrfu_setup/nrfu_setup.html","title":"NRFU Testing","text":"<p>We also provide specific NRFU functionality which allows users to run NRFU tests against their network by using Vane. This functionality can be utilized by connecting Vane to the network in 3 different ways as follows:</p> <ul> <li> <p>Running Vane locally</p> <ol> <li>Connecting to devices locally (via a device ip text file)</li> <li>Connecting to devices via a CVP instance (by providing the CVP ip)</li> </ol> </li> <li> <p>Running Vane as a CVP Application</p> <ol> <li>Connecting to devices by running Vane as a CVP container</li> </ol> </li> </ul>"},{"location":"nrfu_setup/nrfu_setup.html#nrfu-test-cases-supported-in-vane","title":"NRFU Test cases supported in Vane","text":"<p>Note</p> <p>The list of supported NRFU test cases can be found here</p>"},{"location":"nrfu_setup/nrfu_setup.html#i-running-vane-locally","title":"I. Running Vane locally","text":"<p>Executing the below command will prompt for EOS device/CVP credentials (which should be the same)</p> <pre><code>vane --nrfu\n</code></pre> <p>After entering those, you will be prompted as follows:</p> <pre><code>Do you wish to use CVP for DUTs?\n</code></pre>"},{"location":"nrfu_setup/nrfu_setup.html#a-local-execution-without-cvp","title":"a. Local Execution without CVP","text":"<p>If you answer No, it will prompt you as follows:</p> <pre><code>Please input Name/Path of device list file (.txt) (Use tab for autocompletion):\n</code></pre> <p>Once you enter the path to a file with device ip\u2019s, it will prompt you for a test directory as follows:</p> <pre><code>Do you want to specify a test case directory [y|n]: \n</code></pre> <p>If you answer n/no, it will default to the nrfu_tests folder in vane, if yes it will further prompt you as follows:</p> <pre><code>Please specify test case directory &lt;path/to/test case dir&gt; (Use tab for autocompletion):\n</code></pre> <p>Success</p> <p>Following which Vane will start executing the nrfu test cases on the devices whose ip\u2019s were provided in the device file.</p>"},{"location":"nrfu_setup/nrfu_setup.html#b-local-execution-with-cvp","title":"b. Local Execution with CVP","text":"<p>If you answer Yes, it will prompt you as follows:</p> <pre><code>Please input CVP IP address\n</code></pre> <p>Given you authenticate in successfully after providing the address, it will prompt you for the test directory as earlier.</p> <p>Success</p> <p>After which Vane will start executing the test cases on your devices managed through CVP.</p>"},{"location":"nrfu_setup/nrfu_setup.html#ii-running-vane-as-a-cvp-application","title":"II.  Running Vane as a CVP Application","text":"<p>Note</p> <p>If you have the initialized vane-cvp setup, then skip to the second part of Step 7, if not then follow from Step 1</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-1-download-the-vane-cvp-rpm-from-the-vane-repo","title":"Step 1: Download the vane-cvp-rpm from the vane repo","text":"<p>To download the vane-cvp-rpm, select the most recent successful build, which would be the one at the top with a green checkmark, over here.</p> <p>Reference</p> <p></p> <p>Then download the rpm from the artifacts section</p> <p>Note</p> <p>You need to be signed into your github account to be able to download this</p> <p>The package is downloaded as a zip archive.</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-2-scp-the-zipped-rpm-file-from-your-local-device-to-your-cvp-instance","title":"Step 2: scp the zipped rpm file from your local device to your cvp instance","text":"<p>The file should be copied into the root directory on the cvp instance.</p> <pre><code>scp /file/path/vane-cvp-&lt;version-info&gt;.zip root@&lt;cvp_ip&gt;:/root\n</code></pre> <p>Example</p> <p>scp /Users/rewati/Downloads/vane-cvp-1.1.0rc2-rpm.zip root@10.255.67.157:/root</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-3-ssh-into-cvp-instance","title":"Step 3: Ssh into cvp instance","text":"<p>Log into the cvp instance as the root user.</p> <pre><code>ssh root@&lt;cvp_ip&gt;\n</code></pre>"},{"location":"nrfu_setup/nrfu_setup.html#step-4-unzip-and-untar-the-copied-package-in-the-root-users-home-directory","title":"Step 4: Unzip and untar the copied package in the root user's home directory","text":"<p>Tip</p> <p>Use tab completion after typing the unzip command, then use a wildcard on the filename for the tar command to avoid having to type out or copy the filename correctly, since the .tgz file is slightly different than the zip file name.</p> <p>The command and its output will look as follow:</p> <pre><code>[root@&lt;cvp_ip&gt; ~]# unzip vane-cvp-1.1.0rc2-rpm.zip &amp;&amp; tar xzvf vane-cvp*.tgz\nArchive:  vane-cvp-1.1.0rc2-rpm.zip\ninflating: vane-cvp-1.1.0rc2.tgz\nvane-cvp-1.1.0rc2/\nvane-cvp-1.1.0rc2/vane-cvp-install.sh\nvane-cvp-1.1.0rc2/vane-cvp-uninstall.sh\nvane-cvp-1.1.0rc2/vane-cvp-start.sh\nvane-cvp-1.1.0rc2/vane-cvp-1.1.0rc2-1.noarch.rpm\n</code></pre>"},{"location":"nrfu_setup/nrfu_setup.html#step-5-change-to-the-newly-created-directory","title":"Step 5: Change to the newly created directory","text":"<p>Inside the directory, there will be the vane-cvp rpm package and 3 executable shell scripts.</p> <pre><code>[root@&lt;cvp_ip&gt; ~]# cd vane-cvp-1.1.0rc2\n[root@&lt;cvp_ip&gt; vane-cvp-1.1.0rc2]# ls -1\nvane-cvp-1.1.0rc2-1.noarch.rpm\nvane-cvp-install.sh\nvane-cvp-start.sh\nvane-cvp-uninstall.sh\n</code></pre>"},{"location":"nrfu_setup/nrfu_setup.html#step-6a-run-the-vane-cvp-installsh-script-to-install-vane-and-start-the-container","title":"Step 6a: Run the vane-cvp-install.sh script to install vane and start the container","text":"<p>This installs the RPM, enables the extension in CVP, starts the extension and checks the status of the extension, then disables the extension.</p> <p>Note</p> <p>Disabling the extension does not stop the extension from running. It is still active and can be used. Disabling the extension prevents the extension from starting automatically the next time CVP is stopped and restarted, e.g. after a reboot or a manual stop and restart of CVP. This is to help prevent the Vane extension from loading after a reboot, which should help prevent issues during a CVP upgrade.</p> <p>Note</p> <p>Although the extension is disabled and this should help during a CVP upgrade process, the suggested process is to always uninstall the Vane CVP extension see step 6c before proceeding with the CVP upgrade.</p> <pre><code>[root@&lt;cvp_ip&gt; vane-cvp-1.1.0rc2]# ./vane-cvp-install.sh\n\n--------------------------------------------------------------\n\nInstall the rpm\n  ....\n\n&lt;lots of output removed&gt;\n\n  ....\n\n\nAction Output\n-------------\nCOMPONENT     ACTION        NODE      STATUS      ERROR\nvane-cvp      disable      primary   (E) DONE       -\n\n--------------------------------------------------------------\n\n\n-- Vane CVP extension installed --\n</code></pre>"},{"location":"nrfu_setup/nrfu_setup.html#step-6b-restarting-the-vane-extension-after-a-cvp-shutdown-optional","title":"Step 6b: Restarting the Vane extension after a CVP shutdown (optional)","text":"<p>If the CVP instance is stopped for any reason (reboot, manually stopped by cvpi commands, etc), the Vane extension will not be restarted when CVP is restarted. To restart the Vane extension after CVP is running again, cd into the same directory where the rpm package and the 3 shell scripts are located and run the vane-cvp-start.sh script. This script is nearly identical to the vane-cvp-install.sh script with the exception of not performing the rpm installation. It enables and starts the extension, then verifies the status of the extension, and finally disables (without stopping) the extension, as before.</p> <pre><code>cd path_to_directory_containing_rpm_package\n\n./vane-cvp-start.sh\n</code></pre> <p>Warning</p> <p>This step assumes that the extension has been previously installed by Step 6a successfully, and this is only to restart the extension.</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-6c-uninstalling-the-vane-extension-optional","title":"Step 6c: Uninstalling the Vane extension (optional)","text":"<p>Uninstalling the Vane CVP extension is highly recommended before performing an upgrade to either the extension itself or to CVP.</p> <p>To uninstall the extension, cd into the same directory where the rpm package and the 3 shell scripts are located and run the vane-cvp-uninstall.sh script. This script will stop and disable the extension if it is running, remove any containers associated with the extension, and uninstall the extension from the CVP subsystem.</p> <pre><code>cd path_to_directory_containing_rpm_package\n\n./vane-cvp-uninstall.sh\n</code></pre> <p>Note</p> <p>The /cvpi/apps/vane-cvp/vane-data directory that is associated with the container is not removed, so any data that you may have stored in the shared location is still preserved.</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-7-get-a-command-shell-in-the-vane-cvp-extension-container","title":"Step 7: Get a command shell in the vane-cvp extension container","text":"<p>To run Vane, we need to be in the container that is running the vane-cvp extension. To enter the vane-cvp container, exec into the container using kubectl.</p> <pre><code>kubectl exec -it &lt;vane-cvp-container-id&gt; -- /bin/bash\n</code></pre> <p>Tip</p> <p>To get the vane-cvp-container-id start typing \u2018vane\u2019 and hit tab to trigger the tab-completion which should fill in the name of the vane-cvp container. The container id should be something like vane-cvp-667699998-m42gh.</p> <p>If returning to a previously initialized vane-cvp setup, you will need to first log into the CVP host, before exec\u2019ing into the vane-cvp container.</p> <pre><code>ssh root@&lt;cvp_ip&gt;\nkubectl exec -it &lt;vane-cvp-container-id&gt; -- /bin/bash\n</code></pre> <p>Warning</p> <p>If the tab completion doesn't work, an alternative form of the kubectl exec command can be run as</p> <p><pre><code>kubectl exec -it `kubectl get pods | grep vane-cvp | awk '{print $1}'` -- /bin/bash\n</code></pre> where the commands in the backticks will dynamically get the container id.</p>"},{"location":"nrfu_setup/nrfu_setup.html#step-8-activate-vane-shell","title":"Step 8: Activate Vane shell","text":"<p>After the above command, if everything is successful you will be greeted with a prompt as follows, after which if you type activate, you will enter the vane virtual environment</p> <p></p>"},{"location":"nrfu_setup/nrfu_setup.html#step-9-executing-vane","title":"Step 9: Executing Vane","text":"<p>Now you are good to run Vane on the Nrfu tests</p> <pre><code>vane --nrfu\n</code></pre> <p>Executing the above command will prompt for EOS device/CVP credentials (which should be the same)</p> <p>After entering those, it will identify that you are running Vane as a CVP application and show the following prompt</p> <pre><code>Using CVP to gather duts data\n</code></pre> <p>Given CVP authentication is successful, it will prompt you the following:</p> <pre><code>Do you want to specify a test case directory [y|n]: \n</code></pre> <p>If you answer n/no, it will default to the nrfu_tests folder in vane, if yes it will further prompt you</p> <pre><code>Please specify test case directory &lt;path/to/test case dir&gt; (Use tab for autocompletion):\n</code></pre> <p>Tip</p> <p>Look into this section to learn how you can import your own customized, nrfu/non-nrfu, test cases in this working environment</p> <p>Success</p> <p>After which Vane will start executing the test cases on your devices managed through CVP</p>"},{"location":"test_case_style_guide/sample_network_tests.html","title":"Sample Network Test Cases","text":"<p>Help</p> <p>We have sample network test cases which can be used to test the basics of your network and also as a template to write customized test cases. The sample test cases can be found here</p> <p></p>"},{"location":"test_case_style_guide/test_case_setup.html","title":"Vane Test Case Setup","text":""},{"location":"test_case_style_guide/test_case_setup.html#getting-started-with-writing-your-own-test-cases","title":"Getting Started with writing your own test cases","text":"<p>In this section, we will delve into various components essential for crafting your own Vane test case.</p>"},{"location":"test_case_style_guide/test_case_setup.html#i-creating-a-test-definitions-file","title":"I. Creating a test definitions file","text":"<p>The test case definition file is a yaml file that resides in the same folder as the test. Test case input is described in the test case definition file. Below are the fields that make up a test case definition and what they should be used for.</p> <p>Example</p> Sample test_definitions.yaml<pre><code>- name: test_memory.py\ntestcases:\n    - name: test_memory_utilization_on_\n    test_id: TN1.1\n    description: Verify memory is not exceeding high utilization\n    show_cmd: show version\n    # memory process ceiling\n    expected_output: 80\n    report_style: modern\n    test_criteria: Verify memory is not exceeding high utilization\n    # Optionally filter duts by criteria: name, role, regex, or names\n    criteria: names\n    filter: \n        - DSR01\n        - DCBBW1\n    comment: null\n</code></pre> Field Required Description name Yes Name of the test suite testcases Yes List of test case definitions. One per test case. Note this is one per test case not per test suite. For example, if test_memory.py has 5 testcases, a test case definition for each of the test cases is required. name Yes Name of test case. This will appear in the test report. Use underscores to separate words and Vane will remove them when publishing a report. test_id Yes Unique identifier for test case. This will be published in a test report. description Yes Describes the purpose of the test case and what it's testing. This will be published in a test report. show_cmd No Some test cases have simple logic. These test cases send a show command to EOS and then validate a field in the operational data. For efficiency these commands can be run in batch before test case execution. This field inputs commands needing batch execution. If show output is available, Vane will report the show command and its output in human readable text. expected_output No User defined output to be used for test validation. This will vary from test to test and may have many key value pairs. It is not a required field for test cases that do not need a configurable test criteria. It can be published to the test report depending on report template report_style No Reporting template to use when creating output. Vane will default to original reporting without it. test criteria Yes Details which the test case will use to determine pass or fail. This will be published in a test report. criteria No Criteria on the basis of which duts can be filtered. Eg: Name, Role filter No Values which would get filtered via the criteria mentioned above. comment No Additional information about the test case. It can be published to the test report depending on the report template. Example comment is a test case that cannot run because a requirement is not met.  This could be a hardware test for vEOS instance, or not having expected software configurations like TACACS not being configured for TACACS test. <p>Note</p> <p>There are other fields as well which can be explored by viewing the sample network tests</p>"},{"location":"test_case_style_guide/test_case_setup.html#ii-creating-a-test-case-file","title":"II. Creating a test case file","text":"<p>Warning</p> <p>The aim of this section is to help you get started with writing a test case that would make use of a variety of features that Vane provides. There are no hard and fast rules, and user should practice their discretion while making the choice depending on the testing use case. We will try diving a sample test case into sections and briefly describe what each section achieves.</p> <p>Example Test Case File</p> test_memory.py <pre><code>\"\"\" Tests to validate memory utilization.\"\"\"\n\nimport pytest\nfrom pyeapi.eapilib import EapiError\nfrom vane import tests_tools\nfrom vane import test_case_logger\nfrom vane.config import dut_objs, test_defs\n\n\nTEST_SUITE = \"test_memory.py\"\nLOG_FILE = {\"parameters\": {\"show_log\": \"show_output.log\"}}\n\ndut_parameters = tests_tools.parametrize_duts(TEST_SUITE, test_defs, dut_objs)\ntest1_duts = dut_parameters[\"test_memory_utilization_on_\"][\"duts\"]\ntest1_ids = dut_parameters[\"test_memory_utilization_on_\"][\"ids\"]\n\nlogging = test_case_logger.setup_logger(__file__)\n\n\n@pytest.mark.demo\n@pytest.mark.nrfu\n@pytest.mark.platform_status\n@pytest.mark.memory\n@pytest.mark.virtual\n@pytest.mark.physical\nclass MemoryTests:\n    \"\"\"Memory Test Suite\"\"\"\n\n    @pytest.mark.parametrize(\"dut\", test1_duts, ids=test1_ids)\n    def test_memory_utilization_on_(self, dut, tests_definitions):\n        \"\"\"TD: Verify memory is not exceeding high utilization\n\n        Args:\n            dut (dict): Encapsulates dut details including name, connection\n            tests_definitions (dict): Test parameters\n        \"\"\"\n\n        tops = tests_tools.TestOps(tests_definitions, TEST_SUITE, dut)\n\n        try:\n            \"\"\"\n            TS: Run show command 'show version' on dut\n            \"\"\"\n            self.output = dut[\"output\"][tops.show_cmd][\"json\"]\n            assert self.output, \"Memory details are not collected.\"\n            logging.info(\n                f\"On device {tops.dut_name} output of\n                {tops.show_cmd} command is: {self.output}\"\n            )\n\n            memory_total = self.output[\"memTotal\"]\n            memory_free = self.output[\"memFree\"]\n            tops.actual_output = (float(memory_free) /\n            float(memory_total)) * 100\n\n        except (AssertionError, AttributeError, LookupError, EapiError)\n        as exception:\n            logging.error(\n                f\"Error occurred during the testsuite execution on dut: \"\n                f\"{tops.dut_name} is {str(exception)}\"\n            )\n            tops.actual_output = str(exception)\n\n        if tops.actual_output &lt; tops.expected_output:\n            tops.test_result = True\n            tops.output_msg = (\n                f\"On router {tops.dut_name} memory utilization\n                percent is \"\n                f\"{tops.actual_output}% which is correct as it is \"\n                f\"under {tops.expected_output}%\"\n            )\n        else:\n            tops.test_result = False\n            tops.output_msg = (\n                f\"On router {tops.dut_name} the actual memory\n                utilization percent is \"\n                f\"{tops.actual_output}% while it should be under \"\n                f\"{tops.expected_output}%\"\n            )\n\n        tops.parse_test_steps(self.test_memory_utilization_on_)\n        tops.generate_report(tops.dut_name, self.output)\n        assert tops.actual_output &lt; tops.expected_output\n</code></pre>"},{"location":"test_case_style_guide/test_case_setup.html#import-modules","title":"Import modules","text":"<p>Include modules such as pytest, eapi, and other relevant tools from the Vane library like tests tools, test case logger, and dut objects to be utilized in the test case.</p> <pre><code>import pytest\nfrom pyeapi.eapilib import EapiError\nfrom vane import tests_tools\nfrom vane import test_case_logger\nfrom vane.config import dut_objs, test_defs\n</code></pre>"},{"location":"test_case_style_guide/test_case_setup.html#parameterization-of-test-cases","title":"Parameterization of Test Cases","text":"<p>All test cases should either be parametrized or should use parameterized vane fixture. dut is one such parameterized vane fixture.</p> <p>Parameterized test cases solve the problem of grouping duts based on name(s), regex, or role. It can be easily extended to any other dut property defined in duts.yaml. The test definition file will be used to express a filtering criteria and a filter. 2 new key, value pairs (criteria, filter) will be introduced into a test definition for this. These values are optional.</p> key type (of the value) value criteria string Specifies the filtering criteria.  Valid criteria are: name (scenario 1), role (scenario 2), names (scenario 3), regex (scenario 4), roles (scenario 5). If the criteria field is empty or does not match a valid criteria, all duts will be tested (scenario 6). Scenarios are shown below. filter string Filter based on a DUT name. There must be an exact match between the DUT\u2019s name in the duts.yaml file. filter string Filter based on a role name. There must be an exact match between the role\u2019s name in the duts.yaml file. filter list Filter based on a list of roles. There must be an exact match between each role and the duts.yaml file. filter list Filter based on a list of DUT names. There must be an exact match between each DUT\u2019s name and the duts.yaml file. filter string Filter based on a regular expression. Regular expression  will match all DUT\u2019s names in the duts.yaml file that are valid. <p>Six current scenarios exists for filtering DUTs:</p> <ul> <li> <p>Scenario 1: Filter a single DUT named BLW1</p> <pre><code>- name: test_1_sec_cpu_utlization_on_\ndescription: Verify 1 second CPU % is under specificied value\nshow_cmd: show processes\nexpected_output: 1\ncriteria: name\nfilter: BLW1\n</code></pre> </li> <li> <p>Scenario 2: Filter all DUTs with role leaf</p> <pre><code>- name: test_1_min_cpu_utlization_on_\ndescription: Verify 1 minute CPU % is under specificied value\nshow_cmd: show processes\nexpected_output: 10\ncriteria: role\nfilter: leaf\n</code></pre> </li> <li> <p>Scenario 3: Filter  multiple DUTs named BLE2, BLW1</p> <pre><code>- name: test_5_min_cpu_utlization_on_\ndescription: Verify 5 minute CPU % is under specificied value\nshow_cmd: show processes\nexpected_output: 10\ncriteria: names\nfilter:\n    - BLE2\n    - BLW1\n</code></pre> </li> <li> <p>Scenario 4: Filter using a regular expression DUTs named BLE1, BLE2</p> <pre><code>- name: test_1_sec_cpu_utlization_on_\ndescription: Verify 1 second CPU % is under specificied value\nshow_cmd: show processes\nexpected_output: 1\ncriteria: name\nregex: BLE[1|2]\n</code></pre> </li> <li> <p>Scenario 5: Filter all DUTs with roles spine and leaf</p> <pre><code>- name: test_1_min_cpu_utlization_on_\ndescription: Verify 1 minute CPU % is under specified value\nshow_cmd: show processes\nexpected_output: 10\ncriteria: role\nfilter: \n    - leaf\n    - spine\n</code></pre> </li> <li> <p>Scenario 6: All DUTs, this is the default setting and no additional   information needs to be added to the test definition file.</p> </li> </ul> <p>Note</p> <p>The following additions will have to be made to the test case file in order to parameterize the test case.</p> <ul> <li> <p>The global duts object (dut_objs) and global test definitions (test_defs)   are required for input.</p> <pre><code>from vane.config import dut_objs, test_defs\n</code></pre> </li> <li> <p>The method parametrize_duts must be run to create the input for a     parameterized test. TEST_SUITE is passed to the method so the test     suite\u2019s definitions can be discovered in test_defs.  Test_defs     contains the filter and criteria key, value pairs for each test case.     The filter is executed against the duts_objs to create a subset of DUTs.</p> <p>The dut_parameters data structure has parameters for all test keys. The data structure is a dictionary and organized by test case name. Each test case name contains the subset of duts and a list dut names. Below shows the data structure:</p> <pre><code>{ \u201ctest case name 1\u201d :\n    \u201cduts\u201d: {subset of duts 1}\n    \u201cids\u201d: [subset of duts names 1]\n},\n{ \u201ctest case name 2\u201d :\n    \u201cduts\u201d: {subset of duts 2}\n    \u201cids\u201d: [subset of duts names 2]\n},\n\u2026\n{ \u201ctest case name N\u201d :\n    \u201cduts\u201d: {subset of duts N}\n    \u201cids\u201d: [subset of duts names N]\n},\n</code></pre> <p>Lines 4, 5 are optional and are added for better readability in the parameterized decorator.</p> <pre><code>TEST_SUITE = \"test_memory.py\"\n\ndut_parameters = tests_tools.parametrize_duts(TEST_SUITE, test_defs, dut_objs)\ntest1_duts = dut_parameters[\"test_memory_utilization_on_\"][\"duts\"]\ntest1_ids = dut_parameters[\"test_memory_utilization_on_\"][\"ids\"]\n</code></pre> </li> <li> <p>A parameterized decorator is added to each test. The second value     is the subsets of duts that the parameterized decorator     will iterate over. This was provided by the dut_parameters     variable. The first value is the name assigned to the dut_parameter     iteration and it is passed to the function definition.     The third value is a list of names PyTest will display on test     case iteration.</p> <pre><code>class MemoryTests:\n\"\"\"Memory Test Suite\"\"\"\n\n@pytest.mark.parametrize(\"dut\", test1_duts, ids=test1_ids)\ndef test_memory_utilization_on_(self, dut, tests_definitions):\n</code></pre> </li> </ul>"},{"location":"test_case_style_guide/test_case_setup.html#creating-test-case-logs","title":"Creating test case logs","text":"<p>Test cases can make use of the logging functionality to make logs of various levels. We provide an inbuilt logger which can be invoked and used as follows. The logs created by it get generated in the logs folder and stored by the test case file name within the outermost cloned vane directory.</p> <pre><code># Import the logger module\nfrom vane import test_case_logger\n\n# Set the logger with the test case file name\nlogging = test_case_logger.setup_logger(__file__)\n\n# Invoke and make the logs\nlogging.info(\"This is an info log\")\nlogging.debug(\"This is a debug log\")\nlogging.warning(\"This is a warning log\")\nlogging.error(\"This is an error log\")\n</code></pre> <p>Note</p> <p>By default the \"debug\" logs do not get logged, but this can be changed by changing the log levels within the test case logger file</p>"},{"location":"test_case_style_guide/test_case_setup.html#using-markers","title":"Using markers","text":"<p>In Pytest, markers are a way to add metadata or labels to your test functions. You can use markers to group tests together. For example, you might have markers like @pytest.mark.nrfu or @pytest.mark.memory to categorize tests based on their purpose.</p> <pre><code>    @pytest.mark.demo\n    @pytest.mark.nrfu\n    @pytest.mark.platform_status\n    @pytest.mark.memory\n    @pytest.mark.virtual\n    @pytest.mark.physical\n    class MemoryTests:\n</code></pre> <p>You can now run specific groups of tests using markers. For example, if you only want to run nrfu tests, you can add the nrfu marker in the definitions.yaml markers field.</p> <p>Tip</p> <p>You can use the --markers flag in Vane to see the markers supported by Vane</p> <p>Markers make it easy to organize and manage your tests, especially in larger test suites where you might have various types of tests with different requirements.</p> <p>Info</p> <p>You can see the official documentation of Pytest Markers here.</p>"},{"location":"test_case_style_guide/test_case_setup.html#integrating-the-test-case-logic","title":"Integrating the test case logic","text":"<p>This is the actual crux of your test case. Its the testing logic which is woven through the test case. It consists of 3 main sections.</p> <p>Note</p> <p>In the example below we are checking if memory utilization is below a certain threshold by using the output from show version command</p> <ul> <li> <p>Gathering and processing test data</p> <p>The tops object consists of all the essential data that gets used during a test case. Look at the TestOps API section to get an idea of the different features available on the tops object.</p> <pre><code>tops = tests_tools.TestOps(tests_definitions, TEST_SUITE, dut)\ntry:\n    \"\"\"\n    TS: Run show command 'show version' on dut\n    \"\"\"\n    self.output = dut[\"output\"][tops.show_cmd][\"json\"]\n    assert self.output, \"Memory details are not collected.\"\n    logging.info(\n        f\"On device {tops.dut_name} output of {tops.show_cmd} command is: {self.output}\"\n    )\n\n    memory_total = self.output[\"memTotal\"]\n    memory_free = self.output[\"memFree\"]\n    tops.actual_output = (float(memory_free) / float(memory_total)) * 100\n\nexcept (AssertionError, AttributeError, LookupError, EapiError) as exception:\n    logging.error(\n        f\"Error occurred during the testsuite execution on dut: \"\n        f\"{tops.dut_name} is {str(exception)}\"\n    )\n    tops.actual_output = str(exception)\n</code></pre> </li> <li> <p>Comparing Actual and Expected data</p> <pre><code>if tops.actual_output &lt; tops.expected_output:\n    tops.test_result = True\n    tops.output_msg = (\n        f\"On router {tops.dut_name} memory utilization percent is \"\n        f\"{tops.actual_output}% which is correct as it is \"\n        f\"under {tops.expected_output}%\"\n    )\nelse:\n    tops.test_result = False\n    tops.output_msg = (\n        f\"On router {tops.dut_name} the actual memory utilization\n        percent is \"\n        f\"{tops.actual_output}% while it should be under \"\n        f\"{tops.expected_output}%\"\n    )\n</code></pre> </li> <li> <p>Asserting test result.</p> <pre><code>assert tops.actual_output &lt; tops.expected_output\n</code></pre> </li> </ul> <p>Note</p> <p>As mentioned before this organization would  differ for each test case and user's discretion should be used while writing this section. The examples above refer to our sample network tests but your specific test case could be very different.</p>"},{"location":"test_case_style_guide/test_case_setup.html#generating-test-case-reports","title":"Generating test case reports","text":"<p>Vane produces diverse reports in formats such as .json, .html, and .docx. By invoking the generate_report api, all pertinent test data is appended to the test object. This data is subsequently utilized when generating documentation reports through a call to the write_results method in vane_cli.py after the test case has finished executing.</p> <p>Additionally, the generate_report method is responsible for generating evidence files that exhibit various show commands and their corresponding outputs from the devices. These evidence files are crafted and stored in the reports/TEST RESULTS folder.</p> <p>Finally, the generate_report method invokes another function to generate HTML reports containing the results of the test cases.</p> Call to generate report in the test casegenerate_report in tests_tools.pywrite_results in vane_cli.py <pre><code>tops.generate_report(tops.dut_name, self.output)\n</code></pre> <pre><code>def generate_report(self, dut_name, output):\n\"\"\"Utility to generate report\n\nArgs:\n    dut_name (str): name of the device\n\"\"\"\nlogging.debug(f\"Output on device {dut_name} after SSH connection is: {output}\")\n\nself.test_parameters[\"comment\"] = self.comment\nself.test_parameters[\"test_result\"] = self.test_result\nself.test_parameters[\"output_msg\"] = self.output_msg\nself.test_parameters[\"actual_output\"] = self.actual_output\nself.test_parameters[\"expected_output\"] = self.expected_output\nself.test_parameters[\"dut\"] = self.dut_name\nself.test_parameters[\"show_cmd\"] = self.show_cmd\nself.test_parameters[\"test_id\"] = self.test_id\nself.test_parameters[\"show_cmd_txts\"] = self._show_cmd_txts\nself.test_parameters[\"test_steps\"] = self.test_steps\nself.test_parameters[\"show_cmds\"] = self._show_cmds\nself.test_parameters[\"skip\"] = self.skip\n\nif str(self.show_cmd_txt):\n    self.test_parameters[\"show_cmd\"] += \":\\n\\n\" + self.show_cmd_txt\n\nself.test_parameters[\"test_id\"] = self.test_id\nself.test_parameters[\"fail_or_skip_reason\"] = \"\"\n\nif not self.test_parameters[\"test_result\"]:\n    self.test_parameters[\"fail_or_skip_reason\"] = self.output_msg\n\nself._html_report()\nself._write_results()\nself._write_text_results()\n</code></pre> <pre><code>def write_results(definitions_file):\n\"\"\"Write results document\n\nArgs:\n    definitions_file (str): Path and name of definition file\n\"\"\"\nlogging.info(\"Using class ReportClient to create vane_report_client object\")\n\nvane_report_client = report_client.ReportClient(definitions_file)\nvane_report_client.write_result_doc()\n</code></pre> <p>Tip</p> <p>You can view the different kinds of reports that Vane generates in the Executing Vane section</p>"},{"location":"test_case_style_guide/test_case_setup.html#generating-test-case-steps","title":"Generating test case steps","text":"<p>Vane offers the capability to record test steps, facilitating later use for reporting. To record and generate test steps within a test case, employ the following syntax.</p> test steps in a test case <pre><code>    @pytest.mark.parametrize(\"dut\", test1_duts, ids=test1_ids)\n    def test_if_hostname_is_correcet_on_(self, dut, tests_definitions):\n        \"\"\"TD: Verify hostname is set on device is correct\n\n        Args:\n        dut (dict): Encapsulates dut details including name, connection\n        tests_definitions (dict): Test parameters\n        \"\"\"\n\n        tops = tests_tools.TestOps(tests_definitions, TEST_SUITE, dut)\n\n        try:\n            \"\"\"\n            TS: Collecting the output of 'show hostname' command from DUT\n            \"\"\"\n            self.output = dut[\"output\"][tops.show_cmd][\"json\"]\n            assert self.output.get(\"hostname\"), \"Show hostname details\n            are not found\"\n            logging.info(\n                f\"On device {tops.dut_name} output of {tops.show_cmd}\n                command is: {self.output}\"\n            )\n\n            tops.expected_output = {\"hostname\": tops.dut_name}\n            tops.actual_output = {\"hostname\": self.output[\"hostname\"]}\n\n        except (AttributeError, LookupError, EapiError) as exp:\n            tops.actual_output = str(exp)\n            logging.error(\n                f\"On device {tops.dut_name}: Error while running\n                testcase on DUT is: {str(exp)}\"\n            )\n            tops.output_msg += (\n                f\" EXCEPTION encountered on device {tops.dut_name}, while \"\n                f\"investigating hostname name. Vane recorded error: {exp} \"\n            )\n\n        \"\"\"\n        TS: Verify LLDP system name\n        \"\"\"\n        if tops.actual_output == tops.expected_output:\n            tops.test_result = True\n            tops.output_msg = (\n                f\"On router {tops.dut_name} the hostname is correctly \"\n                f\"set to {tops.expected_output['hostname']}\"\n            )\n        else:\n            tops.test_result = False\n            tops.output_msg = (\n                f\"On router {tops.dut_name} the hostname is incorrectly \"\n                f\"set to {tops.actual_output['hostname']}.\n                Hostname should be set \"\n                f\"to {tops.expected_output['hostname']}\"\n            )\n\n        \"\"\"\n        TS: Creating test report based on results\n        \"\"\"\n\n        tops.parse_test_steps(self.test_if_hostname_is_correct_on_)\n        tops.generate_report(tops.dut_name, self.output)\n        assert tops.actual_output == tops.expected_output\n</code></pre>"},{"location":"vane_cvp/vane_cvp.html","title":"Vane CVP Application","text":"<p>This application can be installed on your cvp instance in order to conduct network testing. You can run NRFU tests as well as any other kind of tests that have been automated within your test plan.</p>"},{"location":"vane_cvp/vane_cvp.html#i-running-nrfu-tests-on-your-cvp-instance","title":"I. Running NRFU Tests on your CVP instance","text":"<p>To execute the predefined NRFU tests that are available as a part of this application on your cvp instance refer to this section</p>"},{"location":"vane_cvp/vane_cvp.html#ii-running-non-nrfu-tests-on-your-cvp-instance","title":"II. Running non NRFU Tests on your CVP instance","text":"<p>To execute non-NRFU tests, including customized test cases, and network certification test cases, abide by the following instructions:</p>"},{"location":"vane_cvp/vane_cvp.html#step-1-install-vane-on-the-cvp-host","title":"Step 1 - Install Vane on the CVP host","text":"<p>Follow the instructions, specifically the steps from Step 1 to Step 8 laid out in this section of NRFU Testing. Those steps should guide you towards downloading and installing the application on your cvp instance.</p> <p>By the end of Step 8, you should be able to see the following prompt:</p> <p></p>"},{"location":"vane_cvp/vane_cvp.html#step-2-run-vane","title":"Step 2 - Run Vane","text":"<p>Now you are good to execute non Nrfu tests using Vane. There are 2 paths you could follow at this point:</p> <p>A. Run Vane the default way</p> <p>B. Run Vane using the CVP option (recommended for new users)</p> <p>Before we dive into either of these, let us briefly cover how you can import your customized test cases into this working environment. This is essential as by default the application comes pre-installed only with nrfu tests and sample network tests which test basic aspects of your network. These pre-defined tests can be found within the vane-data directory.</p>"},{"location":"vane_cvp/vane_cvp.html#importing-local-customized-test-directories","title":"Importing local customized test directories","text":"<p>If you are not planning on using custom test cases, you may skip this section and move to either A. Run Vane the default way or B. Run Vane using the CVP option.</p> <p>Explaining the mountpoint between CVP host system and Vane-CVP container</p> <p>The Vane-CVP extension has a mountpoint that links a directory from the host OS to within the container OS so we can easily add/retrieve files and data from the container for use with Vane. This mountpoint is located at <code>/cvpi/apps/vane-cvp/vane-data</code> on the CVP host system, and is mounted as the directory <code>/vane-data</code> within the Vane-CVP container.</p> <p>Once the Vane-CVP extension has been installed, this mountpoint is automatically established, and any activity within the two endpoints will be visible on both sides of the mount. If the Vane-CVP extension is uninstalled or stopped and restarted, the mountpoint is reestablished, and the existing files in the CVP host system's <code>/cvpi/apps/vane-cvp/vane-data</code> directory are remounted. No data will be lost by stopping or uninstalling the Vane-CVP extension.</p> <p>When you first connect to the Vane-CVP container after a fresh install, you are placed in the <code>/vane-data</code> directory. If you do a directory listing, you will see a pytest.ini file, and two directories containing nrfu test cases and sample network test cases.</p> <p></p> <p>If you log into the CVP host in a second shell window, and change to the <code>/cvpi/apps/vane-cvp/vane-data</code> directory and do a directory listing, you will see the same file and directories there.</p> <p></p> <p>Now we can upload our own test case files to the CVP host and put them under this directory for Vane to use within the container. In the CVP host system shell, create a new directory to store your custom test cases.</p> <pre><code>[root@jamazan vane-data]# mkdir my-new-vane-tests\n\n[root@jamazan vane-data]# ls -la\ntotal 24\ndrwxrwxrwx  5 cvp  cvp  4096 Apr  5 15:47 .\ndrwxr-xr-x  4 cvp  cvp  4096 Apr  5 15:28 ..\ndrwxr-xr-x  2 root root 4096 Apr  5 15:47 my-new-vane-tests\ndrwxr-xr-x  2 cvp  cvp  4096 Apr  5 15:45 nrfu_tests\n-rw-r--r--  1 cvp  cvp  2171 Apr  5 15:45 pytest.ini\ndrwxr-xr-x 22 cvp  cvp  4096 Apr  5 15:45 sample_network_tests\n</code></pre> <p>And if you check the directory listing from within the container now, you will see the same directory now appears in the container's listing.</p> <pre><code>(vane-cvp-shell) jamazan vane-data # ls -la\ntotal 24\ndrwxrwxrwx    5 cvp      cvp           4096 Apr  5 15:47 .\ndrwxr-xr-x    1 root     root          4096 Apr  5 15:31 ..\ndrwxr-xr-x    2 root     root          4096 Apr  5 15:47 my-new-vane-tests\ndrwxr-xr-x    2 cvp      cvp           4096 Apr  5 15:45 nrfu_tests\n-rw-r--r--    1 cvp      cvp           2171 Apr  5 15:45 pytest.ini\ndrwxr-xr-x   22 cvp      cvp           4096 Apr  5 15:45 sample_network_tests\n</code></pre> <p>Now that you understand the above it is easy to add your custom test cases to the Vane container. Simply upload your test cases to the CVP host, placing them in the subdirectory of <code>/cvpi/apps/vane-cvp/vane-data</code> that you just created. You can do the upload directly to this location or upload to a temporary location and then move the files to this directory afterwards.</p> <p>Steps</p> <ol> <li>Copy the tests from local directory to the mountpoint location we created within our cvp host <pre><code>scp my-vane-tests.tgz root@cvp-host:/cvpi/apps/vane-cvp/vane-data/my-new-vane-tests/.\n</code></pre></li> <li>Log into the CVP host and change to the mountpoint (/cvpi/apps/vane-cvp/vane-data/my-new-vane-tests/) location (not shown)</li> <li>Unpack the test cases <pre><code>tar xzf my-vane-tests.tgz\n</code></pre></li> </ol> <p>Finally, you need to change the ownership of the test cases to the cvp user, since that is the default user inside CVP containers. Without changing the ownership, Vane will not be able to write some logging and reporting data to the directory when it is running the test cases.</p> <pre><code>chown -R cvp /cvpi/apps/vane-cvp/vane-data/my-new-vane-tests\n</code></pre> <p>Warning</p> <p>The cvp user ownership must be maintained at all times within the directory structure of the mountpoint. If new files or directories are added at any time, these must also be changed to the ownership of the cvp user.</p> <p>After transferring and unpackaging your tests, the directory inside the Vane-CVP container will countain all your test cases.</p> <pre><code>(vane-cvp-shell) jamazan vane-data # ls -la my-new-vane-tests/my-vane-tests/\ntotal 316\ndrwxr-xr-x    2 cvp      root          4096 Apr  5 16:46 .\ndrwxr-xr-x    3 cvp      root          4096 Apr  5 16:46 ..\n-rw-r--r--    1 cvp      root          2257 Apr  5 16:46 conftest.py\n-rw-r--r--    1 cvp      root             0 Apr  5 16:46 definitions_nrfu.yaml\n-rw-r--r--    1 cvp      root             0 Apr  5 16:46 duts_nrfu.yaml\n-rw-r--r--    1 cvp      root          1593 Apr  5 16:46 master_def.yaml\n-rw-r--r--    1 cvp      root          4865 Apr  5 16:46 test_bad_syslog_events.py\n&lt; ...directory listing truncated... &gt;\n</code></pre> <p>Important</p> <p>Ensure that your test case folder has a conftest.py file for initializing the basic pytest configs for a valid test run. You can copy the conftest.py file from the nrfu_tests directory located in the vane-data directory into your test case directory. This sample conftest.py file is also available from the Vane GitHub repository here.</p> <p>Tip</p> <p>If desired, the custom test cases can be edited directly on the CVP host by editing the files in the <code>/cvpi/apps/vane-cvp/vane-data</code> path. Any edits made from the CVP host system will reflect immediately in the Vane-CVP container and will be applied the next time the Vane tests are run.</p> <p>Now that you have imported your local customized test cases, you can follow either of the approaches listed below!</p>"},{"location":"vane_cvp/vane_cvp.html#a-run-vane-the-default-way","title":"A. Run Vane the default way","text":"<p>This is basically the conventional way of running Vane where you set up your definitions.yaml file and duts.yaml file with the necessary information. Executing Vane section covers these steps in detail. Keep in mind to modify the test_dirs field within your definitions.yaml file to mention the path to your imported customized test case folder.</p> <p>We recommend using Option B if this is your first time running Vane, since it will initialize the defaults and use your CVP instance to gather duts data for running vane.</p>"},{"location":"vane_cvp/vane_cvp.html#b-run-vane-using-the-cvp-option-recommended-for-new-users","title":"B. Run Vane using the CVP option (recommended for new users)","text":"<p>Here you will be making use of the vane -- cvp option provided within the vane cli. This allows Vane to use CVP for determining test setup options.</p> <pre><code>vane --cvp\n</code></pre> <p>Executing the above command will lead to a prompt asking you to enter your credentials for the cvp monitored devices.</p> <pre><code>Please input Arista device username: cvpadmin\nPlease input Arista device password:\n</code></pre> <p>After successfully authorizing, it will display the following</p> <pre><code>Using CVP to gather duts data\n</code></pre> <p>After which it will ask you if you want to provide custom test cases (it will default to in built nrfu test cases if you say No)</p> <p>If you say Yes to the above prompt, it will further ask you for the path to the imported test cases.</p> <p></p> <p>Success</p> <p>Once you provide a valid path, you are all set and Vane will execute your customized test cases on your devices and generate corresponding reports.</p>"}]}